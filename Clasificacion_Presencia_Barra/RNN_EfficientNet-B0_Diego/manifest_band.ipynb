{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGSWBtJSvGYE"
   },
   "source": [
    "# 2.0 Creación de Manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMapLX8XvLkx"
   },
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_faWrCtvMh0"
   },
   "source": [
    "### Conexión con Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x4NL4NlvtpB"
   },
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests, time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import os, re, glob\n",
    "from astropy.io import fits\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uwLF3kNvOLC"
   },
   "source": [
    "### Paths e Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT    = \"/content/drive/MyDrive/Proyecto_Integrador\"\n",
    "DATA_DIR     = f\"{PROJ_ROOT}/data/images\"\n",
    "MASTER_CSV   = f\"{PROJ_ROOT}/data/dataset.csv\"\n",
    "DATASETS_DIR = f\"{PROJ_ROOT}/Deteccion/datasets\"\n",
    "\n",
    "# Qué descargar:\n",
    "# \"bars_only\"  -> Bars >= 0.50\n",
    "# \"nonbars\"    -> Bars <= 0.25\n",
    "# \"both\"       -> combina ambos subconjuntos\n",
    "MODE = \"bars_only\"\n",
    "\n",
    "THRESH_NEG = 0.25\n",
    "THRESH_POS = 0.50\n",
    "\n",
    "# Bandas a descargar (ópticas de Legacy Survey)\n",
    "BANDS = [\"g\", \"r\", \"z\"]\n",
    "\n",
    "# Capa y tamaño de recorte\n",
    "LAYER = \"ls-dr9\"     # puedes cambiar a \"ls-dr10\" si lo prefieres\n",
    "PIX_SCALE = 0.262    # arcsec/pixel (óptico)\n",
    "CUTOUT_ARCMIN = 0.5  # lado del recorte en arcmin; ej. 0.5' = 30\"\n",
    "SIZE_PIX = int((CUTOUT_ARCMIN*60.0)/PIX_SCALE)\n",
    "SIZE_PIX = max(64, min(SIZE_PIX, 2048))  # límite razonable\n",
    "\n",
    "# Descarga\n",
    "NUM_WORKERS = 8       # hilos en paralelo\n",
    "RETRIES_PER_URL = 2\n",
    "TIMEOUT_SEC = 60\n",
    "SLEEP_BETWEEN = 0.25  # segundos entre reintentos\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DATASETS_DIR, exist_ok=True)\n",
    "print(\"Descargas a:\", DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lalm7IN-wcV5"
   },
   "source": [
    "## Descarga de Imagenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypf9TahWwlH6"
   },
   "source": [
    "### Obtención de Objetos a descargar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(MASTER_CSV)\n",
    "assert {\"name\",\"objra\",\"objdec\",\"Bars\"}.issubset(df.columns), \"Faltan columnas en master\"\n",
    "\n",
    "df[\"image_id\"] = df[\"name\"].astype(str)\n",
    "df[\"ra\"]  = pd.to_numeric(df[\"objra\"], errors=\"coerce\")\n",
    "df[\"dec\"] = pd.to_numeric(df[\"objdec\"], errors=\"coerce\")\n",
    "df[\"Bars\"] = pd.to_numeric(df[\"Bars\"], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"image_id\",\"ra\",\"dec\",\"Bars\"]).drop_duplicates(subset=[\"image_id\"])\n",
    "\n",
    "if MODE == \"bars_only\":\n",
    "    df_sub = df[df[\"Bars\"] >= THRESH_POS].copy()\n",
    "elif MODE == \"nonbars\":\n",
    "    df_sub = df[df[\"Bars\"] <= THRESH_NEG].copy()\n",
    "elif MODE == \"both\":\n",
    "    df_sub = pd.concat([df[df[\"Bars\"] >= THRESH_POS], df[df[\"Bars\"] <= THRESH_NEG]], ignore_index=True).drop_duplicates(subset=[\"image_id\"])\n",
    "else:\n",
    "    raise ValueError(\"MODE inválido\")\n",
    "\n",
    "print(MODE, \"| objetos a descargar:\", len(df_sub))\n",
    "df_sub.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FOncMijwn9I"
   },
   "source": [
    "### Endpoints y Función de Descarga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cutout_urls(ra, dec, band):\n",
    "    # probamos dos endpoints; usaremos el primero que responda 200 con datos\n",
    "    u1 = f\"https://www.legacysurvey.org/viewer/fits-cutout?ra={ra}&dec={dec}&layer={LAYER}&pixscale={PIX_SCALE}&size={SIZE_PIX}&bands={band}\"\n",
    "    u2 = f\"https://www.legacysurvey.org/viewer/cutout.fits?ra={ra}&dec={dec}&layer={LAYER}&pixscale={PIX_SCALE}&size={SIZE_PIX}&bands={band}\"\n",
    "    return [u1, u2]\n",
    "\n",
    "def download_one(name, ra, dec, band, out_dir=DATA_DIR):\n",
    "    out_path = os.path.join(out_dir, f\"{name}_{band}.fits\")\n",
    "    if os.path.exists(out_path) and os.path.getsize(out_path) > 1000:\n",
    "        return (name, band, True, out_path, \"exists\")\n",
    "\n",
    "    urls = cutout_urls(ra, dec, band)\n",
    "    for u in urls:\n",
    "        for _ in range(RETRIES_PER_URL):\n",
    "            try:\n",
    "                resp = requests.get(u, timeout=TIMEOUT_SEC)\n",
    "                if resp.status_code == 200 and resp.content and len(resp.content) > 1000:\n",
    "                    with open(out_path, \"wb\") as f:\n",
    "                        f.write(resp.content)\n",
    "                    return (name, band, True, out_path, \"ok\")\n",
    "            except Exception as e:\n",
    "                last_err = str(e)\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "    return (name, band, False, out_path, last_err if 'last_err' in locals() else \"fail\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekew008Xwt5x"
   },
   "source": [
    "### Ejecución de descarga de las tres bandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jobs = []\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as ex:\n",
    "    for _, row in df_sub.iterrows():\n",
    "        name = str(row[\"image_id\"]); ra = float(row[\"ra\"]); dec = float(row[\"dec\"])\n",
    "        for b in BANDS:\n",
    "            jobs.append(ex.submit(download_one, name, ra, dec, b))\n",
    "\n",
    "ok, fail = 0, 0\n",
    "for fut in tqdm(as_completed(jobs), total=len(jobs)):\n",
    "    name, band, success, path, msg = fut.result()\n",
    "    if success: ok += 1\n",
    "    else: fail += 1\n",
    "\n",
    "print(f\"Descargas terminadas → OK: {ok} | FAIL: {fail}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc6HP9Z7xIJp"
   },
   "source": [
    "### Creación del manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BAND_RE = re.compile(r\"_(g|r|z)(?=\\.fits$)\", re.IGNORECASE)\n",
    "\n",
    "files = glob.glob(os.path.join(DATA_DIR, \"*.fits\"))\n",
    "records = {}\n",
    "for p in files:\n",
    "    base = os.path.basename(p)\n",
    "    m = BAND_RE.search(base.lower())\n",
    "    if not m:\n",
    "        continue\n",
    "    b = m.group(1).lower()\n",
    "    image_id = re.sub(BAND_RE, \"\", base)  # quita _g/_r/_z\n",
    "    image_id = os.path.splitext(image_id)[0]\n",
    "    rec = records.get(image_id, {\"image_id\":image_id, \"path_g\":None,\"path_r\":None,\"path_z\":None,\n",
    "                                 \"hdu_g\":None,\"hdu_r\":None,\"hdu_z\":None,\"format_hint\":\"per_band\",\"raw_path\":None})\n",
    "    rec[f\"path_{b}\"] = p\n",
    "    records[image_id] = rec\n",
    "\n",
    "manifest = pd.DataFrame.from_dict(records, orient=\"index\").reset_index(drop=True)\n",
    "\n",
    "master = pd.read_csv(MASTER_CSV)\n",
    "manifest = manifest[manifest[\"image_id\"].isin(master[\"name\"].astype(str))].reset_index(drop=True)\n",
    "\n",
    "MANIFEST_CSV = f\"{DATASETS_DIR}/manifest.csv\"\n",
    "manifest.to_csv(MANIFEST_CSV, index=False)\n",
    "print(\"Guardado manifest:\", MANIFEST_CSV, \"| filas:\", len(manifest))\n",
    "manifest.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOI1GDOoxRa9"
   },
   "source": [
    "## Procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GKivEPVxWT1"
   },
   "source": [
    "### Merge con los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_CSV = f\"{DATASETS_DIR}/train.csv\"\n",
    "VAL_CSV   = f\"{DATASETS_DIR}/val.csv\"\n",
    "TEST_CSV  = f\"{DATASETS_DIR}/test.csv\"\n",
    "\n",
    "have_splits = all(os.path.exists(p) for p in [TRAIN_CSV, VAL_CSV, TEST_CSV])\n",
    "print(\"¿Hay splits previos?:\", have_splits)\n",
    "\n",
    "man = pd.read_csv(MANIFEST_CSV)\n",
    "\n",
    "def merge_paths(df, man):\n",
    "    df2 = df.merge(man, how=\"left\", on=\"image_id\")\n",
    "    ok_grz = ((~df2[\"path_g\"].isna()) & (~df2[\"path_r\"].isna()) & (~df2[\"path_z\"].isna())).sum()\n",
    "    print(f\"{len(df2)} filas | con g+r+z = {ok_grz}\")\n",
    "    return df2\n",
    "\n",
    "if have_splits:\n",
    "    train = pd.read_csv(TRAIN_CSV)\n",
    "    val   = pd.read_csv(VAL_CSV)\n",
    "    test  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "    train_p = merge_paths(train, man); train_p.to_csv(f\"{DATASETS_DIR}/train_paths.csv\", index=False)\n",
    "    val_p   = merge_paths(val,   man); val_p.to_csv(f\"{DATASETS_DIR}/val_paths.csv\",   index=False)\n",
    "    test_p  = merge_paths(test,  man); test_p.to_csv(f\"{DATASETS_DIR}/test_paths.csv\", index=False)\n",
    "\n",
    "    # Opcional: versiones sólo con GRZ completo\n",
    "    def keep_full_grz(df):\n",
    "        m = (~df[\"path_g\"].isna()) & (~df[\"path_r\"].isna()) & (~df[\"path_z\"].isna())\n",
    "        return df[m].reset_index(drop=True)\n",
    "    keep_full_grz(train_p).to_csv(f\"{DATASETS_DIR}/train_grz.csv\", index=False)\n",
    "    keep_full_grz(val_p).to_csv(f\"{DATASETS_DIR}/val_grz.csv\",   index=False)\n",
    "    keep_full_grz(test_p).to_csv(f\"{DATASETS_DIR}/test_grz.csv\", index=False)\n",
    "    print(\"Generados *_paths.csv y *_grz.csv\")\n",
    "else:\n",
    "    print(\"No hay splits previos; puedes crearlos con la celda alternativa siguiente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJdw0Mq2xZnb"
   },
   "source": [
    "### Revisión del balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATASETS_DIR = \"/content/drive/MyDrive/Proyecto_Integrador/Deteccion/datasets\"\n",
    "\n",
    "train_p = pd.read_csv(f\"{DATASETS_DIR}/train_paths.csv\")\n",
    "val_p   = pd.read_csv(f\"{DATASETS_DIR}/val_paths.csv\")\n",
    "test_p  = pd.read_csv(f\"{DATASETS_DIR}/test_paths.csv\")\n",
    "\n",
    "def keep_full_grz(df):\n",
    "    m = (~df[\"path_g\"].isna()) & (~df[\"path_r\"].isna()) & (~df[\"path_z\"].isna())\n",
    "    return df[m].reset_index(drop=True), (~m).sum()\n",
    "\n",
    "train_grz, miss_tr = keep_full_grz(train_p)\n",
    "val_grz,   miss_va = keep_full_grz(val_p)\n",
    "test_grz,  miss_te = keep_full_grz(test_p)\n",
    "\n",
    "print(\"GRZ completos → train/val/test:\", len(train_grz), len(val_grz), len(test_grz))\n",
    "print(\"Incompletos → train/val/test:\", miss_tr, miss_va, miss_te)\n",
    "\n",
    "print(\"\\nBalance (GRZ):\")\n",
    "for name, d in [(\"train_grz\", train_grz), (\"val_grz\", val_grz), (\"test_grz\", test_grz)]:\n",
    "    print(name)\n",
    "    print(d[\"label_bin\"].value_counts(normalize=True).rename(\"ratio\").sort_index())\n",
    "\n",
    "# Guardar (por si no existen o quieres refrescar)\n",
    "train_grz.to_csv(f\"{DATASETS_DIR}/train_grz.csv\", index=False)\n",
    "val_grz.to_csv(f\"{DATASETS_DIR}/val_grz.csv\", index=False)\n",
    "test_grz.to_csv(f\"{DATASETS_DIR}/test_grz.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHa1OXZXxmC0"
   },
   "source": [
    "Como los datasets estan desbalanceados, toca descargar las galaxias sin barra también"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKC8ph6XxceE"
   },
   "source": [
    "### Muestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, cv2, matplotlib.pyplot as plt, pandas as pd\n",
    "\n",
    "def show_triplet(row, size=256, ncols=3):\n",
    "    paths = [row[\"path_g\"], row[\"path_r\"], row[\"path_z\"]]\n",
    "    imgs = []\n",
    "    for p in paths:\n",
    "        ext = str(p).lower()\n",
    "        if ext.endswith(\".fits\") or ext.endswith(\".fz\"):\n",
    "            from astropy.io import fits\n",
    "            with fits.open(p, memmap=False) as hdul:\n",
    "                # toma primera HDU 2D\n",
    "                for h in hdul:\n",
    "                    if h.data is not None and getattr(h.data,\"ndim\",0)==2:\n",
    "                        arr = h.data.astype(np.float32); break\n",
    "        else:\n",
    "            arr = cv2.imread(p, cv2.IMREAD_UNCHANGED)\n",
    "            if arr.ndim==3: arr = cv2.cvtColor(arr, cv2.COLOR_BGR2GRAY)\n",
    "            arr = arr.astype(np.float32)\n",
    "        lo, hi = np.percentile(arr, [1, 99.5])\n",
    "        arr = np.clip((arr-lo)/(hi-lo+1e-6), 0, 1)\n",
    "        imgs.append(arr)\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    for i, (img, band) in enumerate(zip(imgs, [\"g\",\"r\",\"z\"]), 1):\n",
    "        plt.subplot(1, ncols, i)\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "        plt.title(f\"{band} | {row['image_id']}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Muestra 3 de train_grz\n",
    "sample = train_grz.sample(min(3, len(train_grz)), random_state=0)\n",
    "for _, row in sample.iterrows():\n",
    "    show_triplet(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC5vcpItxfS5"
   },
   "source": [
    "## Descarga de Galaxias sin barra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKMf7POcx64o"
   },
   "source": [
    "### Inputs, endpoints y funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga adicional para galaxias sin barra (Bars ≤ 0.25)\n",
    "\n",
    "PROJ_ROOT   = \"/content/drive/MyDrive/Proyecto_Integrador\"\n",
    "DATA_DIR    = f\"{PROJ_ROOT}/data\"\n",
    "MASTER_CSV  = f\"{PROJ_ROOT}/data/dataset.csv\"\n",
    "\n",
    "LAYER = \"ls-dr9\"\n",
    "PIX_SCALE = 0.262\n",
    "CUTOUT_ARCMIN = 0.5\n",
    "SIZE_PIX = int((CUTOUT_ARCMIN*60.0)/PIX_SCALE)\n",
    "SIZE_PIX = max(64, min(SIZE_PIX, 2048))\n",
    "BANDS = [\"g\",\"r\",\"z\"]\n",
    "\n",
    "RETRIES = 2\n",
    "TIMEOUT = 60\n",
    "SLEEP   = 0.25\n",
    "NWORK   = 8\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(MASTER_CSV)\n",
    "df[\"Bars\"] = pd.to_numeric(df[\"Bars\"], errors=\"coerce\")\n",
    "df[\"image_id\"] = df[\"name\"].astype(str)\n",
    "df = df.dropna(subset=[\"objra\",\"objdec\",\"Bars\"])\n",
    "\n",
    "# Filtramos galaxias sin barra\n",
    "df_no_bar = df[df[\"Bars\"] <= 0.25].copy()\n",
    "print(\"Galaxias sin barra:\", len(df_no_bar))\n",
    "\n",
    "def cutout_urls(ra, dec, band):\n",
    "    u1 = f\"https://www.legacysurvey.org/viewer/fits-cutout?ra={ra}&dec={dec}&layer={LAYER}&pixscale={PIX_SCALE}&size={SIZE_PIX}&bands={band}\"\n",
    "    u2 = f\"https://www.legacysurvey.org/viewer/cutout.fits?ra={ra}&dec={dec}&layer={LAYER}&pixscale={PIX_SCALE}&size={SIZE_PIX}&bands={band}\"\n",
    "    return [u1, u2]\n",
    "\n",
    "def download_one(name, ra, dec, band):\n",
    "    out_path = os.path.join(DATA_DIR, f\"{name}_{band}.fits\")\n",
    "    if os.path.exists(out_path) and os.path.getsize(out_path) > 1000:\n",
    "        return (name, band, True, \"exists\")\n",
    "    for u in cutout_urls(ra, dec, band):\n",
    "        for _ in range(RETRIES):\n",
    "            try:\n",
    "                r = requests.get(u, timeout=TIMEOUT)\n",
    "                if r.status_code == 200 and r.content and len(r.content) > 1000:\n",
    "                    with open(out_path, \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "                    return (name, band, True, \"ok\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(SLEEP)\n",
    "    return (name, band, False, \"fail\")\n",
    "\n",
    "jobs = []\n",
    "with ThreadPoolExecutor(max_workers=NWORK) as ex:\n",
    "    for _, row in df_no_bar.iterrows():\n",
    "        for b in BANDS:\n",
    "            jobs.append(ex.submit(download_one, row[\"image_id\"], row[\"objra\"], row[\"objdec\"], b))\n",
    "\n",
    "ok = fail = 0\n",
    "for fut in as_completed(jobs):\n",
    "    _, _, success, _ = fut.result()\n",
    "    if success: ok += 1\n",
    "    else: fail += 1\n",
    "\n",
    "print(f\"Descargas completadas → OK: {ok}, FAIL: {fail}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZbLgbSRx4p2"
   },
   "source": [
    "### Actualización de manifest y archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, pandas as pd\n",
    "from astropy.io import fits\n",
    "\n",
    "PROJ_ROOT    = \"/content/drive/MyDrive/Proyecto_Integrador\"\n",
    "DATA_DIR     = f\"{PROJ_ROOT}/data\"\n",
    "DATASETS_DIR = f\"{PROJ_ROOT}/Deteccion/datasets\"\n",
    "MASTER_CSV   = f\"{PROJ_ROOT}/data/dataset.csv\"\n",
    "\n",
    "BAND_RE = re.compile(r\"_(g|r|z)(?=\\.fits$)\", re.IGNORECASE)\n",
    "\n",
    "records = {}\n",
    "for p in glob.glob(os.path.join(DATA_DIR, \"*.fits\")):\n",
    "    base = os.path.basename(p).lower()\n",
    "    m = BAND_RE.search(base)\n",
    "    if not m: continue\n",
    "    b = m.group(1)\n",
    "    image_id = re.sub(BAND_RE, \"\", base)\n",
    "    image_id = os.path.splitext(image_id)[0]\n",
    "    rec = records.get(image_id, {\"image_id\":image_id, \"path_g\":None,\"path_r\":None,\"path_z\":None,\n",
    "                                 \"hdu_g\":None,\"hdu_r\":None,\"hdu_z\":None,\"format_hint\":\"per_band\",\"raw_path\":None})\n",
    "    rec[f\"path_{b}\"] = p\n",
    "    records[image_id] = rec\n",
    "\n",
    "manifest = pd.DataFrame.from_dict(records, orient=\"index\").reset_index(drop=True)\n",
    "master = pd.read_csv(MASTER_CSV)\n",
    "manifest = manifest[manifest[\"image_id\"].isin(master[\"name\"].astype(str))].reset_index(drop=True)\n",
    "MANIFEST_CSV = f\"{DATASETS_DIR}/manifest.csv\"\n",
    "manifest.to_csv(MANIFEST_CSV, index=False)\n",
    "print(\"Manifest actualizado:\", MANIFEST_CSV, \"| filas:\", len(manifest))\n",
    "\n",
    "# Recrear *_paths y *_grz\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    base = pd.read_csv(f\"{DATASETS_DIR}/{split}.csv\")\n",
    "    merged = base.merge(manifest, on=\"image_id\", how=\"left\")\n",
    "    merged.to_csv(f\"{DATASETS_DIR}/{split}_paths.csv\", index=False)\n",
    "    m = (~merged[\"path_g\"].isna()) & (~merged[\"path_r\"].isna()) & (~merged[\"path_z\"].isna())\n",
    "    merged[m].to_csv(f\"{DATASETS_DIR}/{split}_grz.csv\", index=False)\n",
    "    print(f\"{split} → total {len(merged)} | GRZ {int(m.sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATASETS_DIR = \"/content/drive/MyDrive/Proyecto_Integrador/Deteccion/datasets\"\n",
    "\n",
    "for split in [\"train_grz\",\"val_grz\",\"test_grz\"]:\n",
    "    df = pd.read_csv(f\"{DATASETS_DIR}/{split}.csv\")\n",
    "    print(f\"{split} ({len(df)} muestras)\")\n",
    "    print(df[\"label_bin\"].value_counts(normalize=True).rename(\"ratio\"), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATASETS_DIR = \"/content/drive/MyDrive/Proyecto_Integrador/Deteccion/datasets\"\n",
    "\n",
    "train_p = pd.read_csv(f\"{DATASETS_DIR}/train_paths.csv\")\n",
    "val_p   = pd.read_csv(f\"{DATASETS_DIR}/val_paths.csv\")\n",
    "test_p  = pd.read_csv(f\"{DATASETS_DIR}/test_paths.csv\")\n",
    "\n",
    "def keep_full_grz(df):\n",
    "    m = (~df[\"path_g\"].isna()) & (~df[\"path_r\"].isna()) & (~df[\"path_z\"].isna())\n",
    "    return df[m].reset_index(drop=True), (~m).sum()\n",
    "\n",
    "train_grz, miss_tr = keep_full_grz(train_p)\n",
    "val_grz,   miss_va = keep_full_grz(val_p)\n",
    "test_grz,  miss_te = keep_full_grz(test_p)\n",
    "\n",
    "print(\"GRZ completos → train/val/test:\", len(train_grz), len(val_grz), len(test_grz))\n",
    "print(\"Incompletos → train/val/test:\", miss_tr, miss_va, miss_te)\n",
    "\n",
    "print(\"\\nBalance (GRZ):\")\n",
    "for name, d in [(\"train_grz\", train_grz), (\"val_grz\", val_grz), (\"test_grz\", test_grz)]:\n",
    "    print(name)\n",
    "    print(d[\"label_bin\"].value_counts(normalize=True).rename(\"ratio\").sort_index())\n",
    "\n",
    "# Guardar (por si no existen o quieres refrescar)\n",
    "train_grz.to_csv(f\"{DATASETS_DIR}/train_grz.csv\", index=False)\n",
    "val_grz.to_csv(f\"{DATASETS_DIR}/val_grz.csv\", index=False)\n",
    "test_grz.to_csv(f\"{DATASETS_DIR}/test_grz.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ag-dPaGykL5"
   },
   "source": [
    "## Comprobación de balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "PROJ_ROOT    = \"/content/drive/MyDrive/Proyecto_Integrador\"\n",
    "DATASETS_DIR = f\"{PROJ_ROOT}/Deteccion/datasets\"\n",
    "DATA_DIR     = f\"{PROJ_ROOT}/data\"\n",
    "MASTER_CSV   = f\"{PROJ_ROOT}/data/dataset.csv\"\n",
    "\n",
    "THRESH_NEG = 0.25\n",
    "THRESH_POS = 0.50\n",
    "R_TRAIN, R_VAL, R_TEST = 0.70, 0.15, 0.15\n",
    "SEED = 42\n",
    "\n",
    "df = pd.read_csv(MASTER_CSV)\n",
    "df[\"image_id\"] = df[\"name\"].astype(str)\n",
    "df[\"Bars\"] = pd.to_numeric(df[\"Bars\"], errors=\"coerce\")\n",
    "\n",
    "def to_bin(x):\n",
    "    if x <= THRESH_NEG: return 0\n",
    "    if x >= THRESH_POS: return 1\n",
    "    return -1\n",
    "\n",
    "df[\"label_bin\"] = df[\"Bars\"].apply(to_bin)\n",
    "df_clean = df[df[\"label_bin\"].isin([0,1])].copy()\n",
    "\n",
    "print(\"Total limpio:\", len(df_clean))\n",
    "print(df_clean[\"label_bin\"].value_counts(normalize=True).rename(\"ratio\"))\n",
    "\n",
    "# --- Split estratificado ---\n",
    "train_df, temp_df = train_test_split(df_clean, test_size=(1-R_TRAIN), random_state=SEED, stratify=df_clean[\"label_bin\"])\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=(1 - (R_VAL/(R_VAL+R_TEST))), random_state=SEED, stratify=temp_df[\"label_bin\"])\n",
    "\n",
    "cols_out = [\"image_id\",\"label_bin\",\"Bars\",\"name\",\"objra\",\"objdec\"]\n",
    "for name, d in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "    d[cols_out].to_csv(f\"{DATASETS_DIR}/{name}.csv\", index=False)\n",
    "    print(f\"{name}: {len(d)} muestras (0/1) →\")\n",
    "    print(d[\"label_bin\"].value_counts(normalize=True).rename(\"ratio\"), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "DATASETS_DIR = \"/content/drive/MyDrive/Proyecto_Integrador/Deteccion/datasets\"\n",
    "MANIFEST_CSV = f\"{DATASETS_DIR}/manifest.csv\"\n",
    "\n",
    "man = pd.read_csv(MANIFEST_CSV)\n",
    "\n",
    "def merge_paths(df, man):\n",
    "    df2 = df.merge(man, how=\"left\", on=\"image_id\")\n",
    "    ok_grz = ((~df2[\"path_g\"].isna()) & (~df2[\"path_r\"].isna()) & (~df2[\"path_z\"].isna())).sum()\n",
    "    print(f\"{len(df2)} filas | con GRZ = {ok_grz}\")\n",
    "    return df2\n",
    "\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    df = pd.read_csv(f\"{DATASETS_DIR}/{split}.csv\")\n",
    "    dfp = merge_paths(df, man)\n",
    "    dfp.to_csv(f\"{DATASETS_DIR}/{split}_paths.csv\", index=False)\n",
    "\n",
    "    # Crear versión sólo con GRZ completo\n",
    "    m = (~dfp[\"path_g\"].isna()) & (~dfp[\"path_r\"].isna()) & (~dfp[\"path_z\"].isna())\n",
    "    dfp[m].to_csv(f\"{DATASETS_DIR}/{split}_grz.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train_grz\",\"val_grz\",\"test_grz\"]:\n",
    "    df = pd.read_csv(f\"{DATASETS_DIR}/{split}.csv\")\n",
    "    print(f\"{split} ({len(df)} muestras)\")\n",
    "    print(df[\"label_bin\"].value_counts(normalize=True).rename(\"ratio\"), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os, re, requests, time, glob\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "PROJ_ROOT    = \"/content/drive/MyDrive/Proyecto_Integrador\"\n",
    "DATA_DIR     = f\"{PROJ_ROOT}/data\"\n",
    "DATASETS_DIR = f\"{PROJ_ROOT}/Deteccion/datasets\"\n",
    "MASTER_CSV   = f\"{PROJ_ROOT}/data/dataset.csv\"\n",
    "MANIFEST_CSV = f\"{DATASETS_DIR}/manifest.csv\"\n",
    "\n",
    "LAYER = \"ls-dr9\"\n",
    "PIX_SCALE = 0.262\n",
    "CUTOUT_ARCMIN = 0.5\n",
    "SIZE_PIX = int((CUTOUT_ARCMIN*60.0)/PIX_SCALE)\n",
    "SIZE_PIX = max(64, min(SIZE_PIX, 2048))\n",
    "BANDS = [\"g\",\"r\",\"z\"]\n",
    "RETRIES = 2\n",
    "TIMEOUT = 60\n",
    "SLEEP = 0.25\n",
    "NWORK = 8\n",
    "\n",
    "# --- 1) Leer manifest y master ---\n",
    "manifest = pd.read_csv(MANIFEST_CSV)\n",
    "master = pd.read_csv(MASTER_CSV)\n",
    "master[\"image_id\"] = master[\"name\"].astype(str)\n",
    "master[\"Bars\"] = pd.to_numeric(master[\"Bars\"], errors=\"coerce\")\n",
    "df_bar = master[master[\"Bars\"] >= 0.5].copy()\n",
    "\n",
    "# --- 2) Encontrar cuáles faltan ---\n",
    "need = []\n",
    "for _, row in df_bar.iterrows():\n",
    "    img_id = str(row[\"image_id\"])\n",
    "    subset = manifest[manifest[\"image_id\"] == img_id]\n",
    "    if subset.empty:\n",
    "        for b in BANDS: need.append((img_id, row[\"objra\"], row[\"objdec\"], b))\n",
    "    else:\n",
    "        for b in BANDS:\n",
    "            if pd.isna(subset.iloc[0].get(f\"path_{b}\")):\n",
    "                need.append((img_id, row[\"objra\"], row[\"objdec\"], b))\n",
    "\n",
    "print(f\"Galaxias con barra a completar: {len(set([n[0] for n in need]))}\")\n",
    "print(\"Total descargas necesarias:\", len(need))\n",
    "\n",
    "# --- 3) Función de descarga ---\n",
    "def cutout_urls(ra, dec, band):\n",
    "    u1 = f\"https://www.legacysurvey.org/viewer/fits-cutout?ra={ra}&dec={dec}&layer={LAYER}&pixscale={PIX_SCALE}&size={SIZE_PIX}&bands={band}\"\n",
    "    u2 = f\"https://www.legacysurvey.org/viewer/cutout.fits?ra={ra}&dec={dec}&layer={LAYER}&pixscale={PIX_SCALE}&size={SIZE_PIX}&bands={band}\"\n",
    "    return [u1, u2]\n",
    "\n",
    "def dl_one(img_id, ra, dec, band):\n",
    "    out_path = os.path.join(DATA_DIR, f\"{img_id}_{band}.fits\")\n",
    "    if os.path.exists(out_path) and os.path.getsize(out_path) > 1000:\n",
    "        return (img_id, band, True, \"exists\")\n",
    "    for u in cutout_urls(ra, dec, band):\n",
    "        for _ in range(RETRIES):\n",
    "            try:\n",
    "                r = requests.get(u, timeout=TIMEOUT)\n",
    "                if r.status_code==200 and r.content and len(r.content)>1000:\n",
    "                    with open(out_path, \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "                    return (img_id, band, True, \"ok\")\n",
    "            except Exception as e:\n",
    "                last = str(e)\n",
    "            time.sleep(SLEEP)\n",
    "    return (img_id, band, False, last if 'last' in locals() else \"fail\")\n",
    "\n",
    "# --- 4) Descargar en paralelo ---\n",
    "jobs=[]\n",
    "with ThreadPoolExecutor(max_workers=NWORK) as ex:\n",
    "    for (img_id, ra, dec, band) in need:\n",
    "        jobs.append(ex.submit(dl_one, img_id, ra, dec, band))\n",
    "\n",
    "ok=fail=0\n",
    "for fut in as_completed(jobs):\n",
    "    _,_,s,_=fut.result()\n",
    "    ok += int(s)\n",
    "    fail += int(not s)\n",
    "\n",
    "print(\"Descargas completadas: OK =\", ok, \"| FAIL =\", fail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, pandas as pd\n",
    "from astropy.io import fits\n",
    "\n",
    "PROJ_ROOT    = \"/content/drive/MyDrive/Proyecto_Integrador\"\n",
    "DATA_DIR     = f\"{PROJ_ROOT}/data\"\n",
    "DATASETS_DIR = f\"{PROJ_ROOT}/Deteccion/datasets\"\n",
    "MASTER_CSV   = f\"{PROJ_ROOT}/data/dataset.csv\"\n",
    "\n",
    "BAND_RE = re.compile(r\"_(g|r|z)(?=\\.fits$)\", re.IGNORECASE)\n",
    "\n",
    "records = {}\n",
    "files = glob.glob(os.path.join(DATA_DIR, \"*.fits\"))\n",
    "for p in files:\n",
    "    base = os.path.basename(p).lower()\n",
    "    m = BAND_RE.search(base)\n",
    "    if not m:\n",
    "        continue\n",
    "    b = m.group(1)\n",
    "    image_id = re.sub(BAND_RE, \"\", base)\n",
    "    image_id = os.path.splitext(image_id)[0]\n",
    "    rec = records.get(image_id, {\n",
    "        \"image_id\":image_id, \"path_g\":None,\"path_r\":None,\"path_z\":None,\n",
    "        \"hdu_g\":None,\"hdu_r\":None,\"hdu_z\":None,\"format_hint\":\"per_band\",\"raw_path\":None\n",
    "    })\n",
    "    rec[f\"path_{b}\"] = os.path.join(DATA_DIR, os.path.basename(p))\n",
    "    records[image_id] = rec\n",
    "\n",
    "manifest = pd.DataFrame.from_dict(records, orient=\"index\").reset_index(drop=True)\n",
    "\n",
    "# Mantener sólo ids presentes en master (seguridad)\n",
    "master = pd.read_csv(MASTER_CSV)\n",
    "manifest = manifest[manifest[\"image_id\"].isin(master[\"name\"].astype(str))].reset_index(drop=True)\n",
    "\n",
    "MANIFEST_CSV = f\"{DATASETS_DIR}/manifest.csv\"\n",
    "manifest.to_csv(MANIFEST_CSV, index=False)\n",
    "print(\"Manifest actualizado:\", MANIFEST_CSV, \"| filas:\", len(manifest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "DATASETS_DIR = \"/content/drive/MyDrive/Proyecto_Integrador/Deteccion/datasets\"\n",
    "MANIFEST_CSV = f\"{DATASETS_DIR}/manifest.csv\"\n",
    "\n",
    "man = pd.read_csv(MANIFEST_CSV)\n",
    "\n",
    "def merge_paths(df, man):\n",
    "    df2 = df.merge(man, how=\"left\", on=\"image_id\")\n",
    "    ok_grz = ((~df2[\"path_g\"].isna()) & (~df2[\"path_r\"].isna()) & (~df2[\"path_z\"].isna())).sum()\n",
    "    print(f\"{len(df2)} filas | con GRZ = {ok_grz}\")\n",
    "    return df2\n",
    "\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    base = pd.read_csv(f\"{DATASETS_DIR}/{split}.csv\")\n",
    "    merged = merge_paths(base, man)\n",
    "    merged.to_csv(f\"{DATASETS_DIR}/{split}_paths.csv\", index=False)\n",
    "\n",
    "    m = (~merged[\"path_g\"].isna()) & (~merged[\"path_r\"].isna()) & (~merged[\"path_z\"].isna())\n",
    "    merged[m].to_csv(f\"{DATASETS_DIR}/{split}_grz.csv\", index=False)\n",
    "    print(f\"{split} → GRZ guardado:\", f\"{DATASETS_DIR}/{split}_grz.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASETS_DIR = \"/content/drive/MyDrive/Proyecto_Integrador/Deteccion/datasets\"\n",
    "for split in [\"train_grz\",\"val_grz\",\"test_grz\"]:\n",
    "    df = pd.read_csv(f\"{DATASETS_DIR}/{split}.csv\")\n",
    "    print(f\"{split} ({len(df)} muestras)\")\n",
    "    print(df[\"label_bin\"].value_counts(normalize=True).rename(\"ratio\").sort_index())\n",
    "    print(\"-\"*40)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
