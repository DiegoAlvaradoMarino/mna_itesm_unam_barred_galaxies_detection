{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Retxa41ZEnXT"
   },
   "source": [
    "# Avance 4: Modelos alternativos\n",
    "\n",
    "**Proyecto:** Detección de barras en galaxias  \n",
    "**Equipo 48**  \n",
    "- A01795687 – Diego Alvarado Marino  \n",
    "- A01795204 – Jonathan Puga Castellanos  \n",
    "- A01381334 – José Antonio Hernández Hernández  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ok29N9_MyyiU"
   },
   "source": [
    "# Modelo 1: Red Neuronal Convolucional - EfficientNet-B0 | 5.3 M Parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV8HrKLqzHQX"
   },
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoQF2PgJzKK0"
   },
   "source": [
    "### Instalación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm torchmetrics albumentations opencv-python astropy grad-cam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP1qOvSnzMj0"
   },
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, json, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryCalibrationError\n",
    "import timm\n",
    "from astropy.io import fits\n",
    "import cv2, numpy as np, pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from astropy.io import fits\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import BinaryClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    average_precision_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AozEnA9lzPiZ"
   },
   "source": [
    "### SEED y GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akR1zCubzTnV"
   },
   "source": [
    "### Conexión con Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UH1qY3fzZik"
   },
   "source": [
    "## Procesamiento de Imagenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YvRbx3CzvSM"
   },
   "source": [
    "### Procesamiento y apilado de imágenes astronómicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_band_image(path):\n",
    "    # Lee FITS o PNG y devuelve float32 en [0,1] con asinh stretch controlado\n",
    "    if path.endswith(\".fits\"):\n",
    "        data = fits.getdata(path).astype(np.float32)\n",
    "        data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    else:\n",
    "        data = cv2.imread(path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "        if data.ndim==3: data = cv2.cvtColor(data, cv2.COLOR_BGR2GRAY)\n",
    "    # recorte de percentiles y asinh\n",
    "    lo, hi = np.percentile(data, [1, 99.5])\n",
    "    if hi<=lo: hi = lo+1e-6\n",
    "    data = np.clip((data - lo) / (hi - lo), 0, 1)\n",
    "    data = np.arcsinh(10*data) / np.arcsinh(10)  # contraste suave\n",
    "    return data\n",
    "\n",
    "def stack_bands(root, image_id, bands=(\"g\",\"r\",\"z\"), ext=\".fits\", size=256):\n",
    "    chans = []\n",
    "    for b in bands:\n",
    "        p = f\"{root}/{b}/{image_id}{ext}\"\n",
    "        if not os.path.exists(p):\n",
    "            p = f\"{root}/{b}/{image_id}.png\"\n",
    "        img = read_band_image(p)\n",
    "        # centra/pad a cuadrado y resize conservador\n",
    "        h,w = img.shape\n",
    "        m = max(h,w)\n",
    "        canvas = np.zeros((m,m), np.float32)\n",
    "        y0 = (m-h)//2; x0 = (m-w)//2\n",
    "        canvas[y0:y0+h, x0:x0+w] = img\n",
    "        img = cv2.resize(canvas, (size,size), interpolation=cv2.INTER_AREA)\n",
    "        chans.append(img)\n",
    "    x = np.stack(chans, axis=0)  #\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlM1Riki0l_q"
   },
   "source": [
    "### Dataset personalizado con imágenes astronómicas y Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _read_fits_first2d(path):\n",
    "    with fits.open(path, memmap=False) as hdul:\n",
    "        for h in hdul:\n",
    "            if h.data is not None and getattr(h.data, \"ndim\", 0)==2:\n",
    "                arr = h.data.astype(np.float32); break\n",
    "    return np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def _read_any_gray(path):\n",
    "    p = str(path).lower()\n",
    "    if p.endswith(\".fits\") or p.endswith(\".fz\"):\n",
    "        return _read_fits_first2d(path)\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None: raise FileNotFoundError(path)\n",
    "    if img.ndim==3: img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "def _stretch_asinh(x):\n",
    "    lo, hi = np.percentile(x, [1, 99.5])\n",
    "    x = np.clip((x-lo)/(hi-lo+1e-6), 0, 1)\n",
    "    return np.arcsinh(10*x)/np.arcsinh(10)\n",
    "\n",
    "def _pad_resize_square(img, size=256):\n",
    "    h,w = img.shape; m = max(h,w)\n",
    "    canvas = np.zeros((m,m), np.float32)\n",
    "    y0=(m-h)//2; x0=(m-w)//2\n",
    "    canvas[y0:y0+h, x0:x0+w] = img\n",
    "    return cv2.resize(canvas, (size,size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def stack_from_row(row, size=256):\n",
    "    chans=[]\n",
    "    for b in [\"g\",\"r\",\"z\"]:\n",
    "        p = row[f\"path_{b}\"]\n",
    "        img = _read_any_gray(p)\n",
    "        img = _pad_resize_square(_stretch_asinh(img), size)\n",
    "        chans.append(img)\n",
    "    return np.stack(chans, axis=0)  # CxHxW\n",
    "\n",
    "def build_transforms(train=True):\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.Rotate(limit=180, border_mode=cv2.BORDER_REFLECT_101, p=1.0),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.0, rotate_limit=0, border_mode=cv2.BORDER_REFLECT_101, p=0.5),\n",
    "            A.GaussianBlur(blur_limit=(3,5), p=0.3),\n",
    "            A.GaussNoise(var_limit=(1e-5,5e-4), p=0.3),\n",
    "            A.RandomBrightnessContrast(0.05,0.05,p=0.3),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([ToTensorV2()])\n",
    "\n",
    "class BarsDatasetPaths(Dataset):\n",
    "    def __init__(self, csv_path, size=256, train=True):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.size = size\n",
    "        self.tfm = build_transforms(train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        x = stack_from_row(r, size=self.size)\n",
    "        x = np.transpose(x, (1,2,0))\n",
    "        x = self.tfm(image=x)[\"image\"]\n",
    "        y_bin = torch.tensor(r.label_bin, dtype=torch.float32)\n",
    "        y_str = torch.tensor(r.Bars,      dtype=torch.float32)\n",
    "        return x, y_bin, y_str, r.image_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BarsDatasetPaths(\"/content/drive/MyDrive/Proyecto_Integrador/Clasificacion_Presencia_Barra/RNN_EfficientNet-B0_Diego/datasets/train_grz.csv\", size=256, train=True)\n",
    "x, yb, ys, _ = ds[0]\n",
    "print(x.shape)  # debe ser torch.Size([3, 256, 256])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrd7YBta1KDg"
   },
   "source": [
    "## Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p5aosbF1eKc"
   },
   "source": [
    "BarNet en PyTorch con EfficientNet como backbone y dos cabezas de predicción (binaria y continua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarNet(nn.Module):\n",
    "    def __init__(self, backbone=\"tf_efficientnet_b0_ns\", in_chans=3, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=True, in_chans=in_chans, drop_rate=drop, drop_path_rate=0.1, num_classes=0, global_pool=\"avg\")\n",
    "        emb = self.backbone.num_features\n",
    "        self.head_bin = nn.Sequential(nn.Linear(emb, 256), nn.ReLU(), nn.Dropout(0.2), nn.Linear(256,1))\n",
    "        self.head_str = nn.Sequential(nn.Linear(emb, 256), nn.ReLU(), nn.Dropout(0.2), nn.Linear(256,1))\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        logit = self.head_bin(feat).squeeze(1)\n",
    "        strength = self.head_str(feat).squeeze(1)\n",
    "        return logit, strength\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ9d-gxZ1kaD"
   },
   "source": [
    "### Pérdidas, Métricas y ciclos de entrenamiento/validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses_and_metrics(logit, strength_pred, y_bin, y_str):\n",
    "    bce = F.binary_cross_entropy_with_logits(logit, y_bin)\n",
    "    huber = F.huber_loss(torch.sigmoid(strength_pred), y_str)\n",
    "    loss = bce + 0.5*huber\n",
    "    prob = torch.sigmoid(logit).detach().cpu().numpy()\n",
    "    yb = y_bin.detach().cpu().numpy()\n",
    "    auprc = average_precision_score(yb, prob) if (yb.min()!=yb.max()) else np.nan\n",
    "    f1 = f1_score((prob>=0.5).astype(int), yb.astype(int)) if (yb.min()!=yb.max()) else np.nan\n",
    "    mae = np.mean(np.abs(torch.sigmoid(strength_pred).detach().cpu().numpy() - y_str.detach().cpu().numpy()))\n",
    "    return loss, {\"auprc\":auprc, \"f1@0.5\":f1, \"mae_str\":mae}\n",
    "\n",
    "def train_one_epoch(model, loader, opt):\n",
    "    model.train()\n",
    "    logs=[]\n",
    "    for x, yb, ys, _ in loader:\n",
    "        x, yb, ys = x.to(device), yb.to(device), ys.to(device)\n",
    "        opt.zero_grad()\n",
    "        logit, sp = model(x)\n",
    "        loss, _ = losses_and_metrics(logit, sp, yb, ys)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        opt.step()\n",
    "        logs.append(loss.item())\n",
    "    return float(np.mean(logs))\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    probs=[]; gts=[]; preds_str=[]; gts_str=[]\n",
    "    for x, yb, ys, _ in loader:\n",
    "        x = x.to(device)\n",
    "        logit, sp = model(x)\n",
    "        probs.append(torch.sigmoid(logit).cpu().numpy())\n",
    "        preds_str.append(torch.sigmoid(sp).cpu().numpy())\n",
    "        gts.append(yb.numpy()); gts_str.append(ys.numpy())\n",
    "    prob = np.concatenate(probs); yb = np.concatenate(gts).astype(np.float32)\n",
    "    sp = np.concatenate(preds_str); ys = np.concatenate(gts_str).astype(np.float32)\n",
    "    auprc = average_precision_score(yb, prob) if (yb.min()!=yb.max()) else float(\"nan\")\n",
    "    f1 = f1_score((prob>=0.5).astype(int), yb.astype(int)) if (yb.min()!=yb.max()) else float(\"nan\")\n",
    "    mae = float(np.mean(np.abs(sp - ys)))\n",
    "    return {\"val_auprc\":auprc, \"val_f1@0.5\":f1, \"val_mae_str\":mae}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y49YPzk-2Atr"
   },
   "source": [
    "### Función de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_paths(csv_train, csv_val, size=256, in_chans=3, epochs=20, lr=2e-4, bs=32,\n",
    "              backbone=\"tf_efficientnet_b0_ns\", out_dir=\"runs/barnet\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    tr_ds = BarsDatasetPaths(csv_train, size=size, train=True)\n",
    "    va_ds = BarsDatasetPaths(csv_val,   size=size, train=False)\n",
    "    tr = DataLoader(tr_ds, batch_size=bs, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "    va = DataLoader(va_ds, batch_size=bs, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = BarNet(backbone=backbone, in_chans=in_chans).to(device)\n",
    "    opt = AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    sch = CosineAnnealingLR(opt, T_max=epochs)\n",
    "    best = -1; patience=6; bad=0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_loss = train_one_epoch(model, tr, opt)\n",
    "        sch.step()\n",
    "        m = validate(model, va)\n",
    "        score = m[\"val_auprc\"]\n",
    "        torch.save({\"model\":model.state_dict(),\"epoch\":ep,\"metrics\":m}, f\"{out_dir}/last.pt\")\n",
    "        if score>best:\n",
    "            best=score; bad=0\n",
    "            torch.save({\"model\":model.state_dict(),\"epoch\":ep,\"metrics\":m}, f\"{out_dir}/best.pt\")\n",
    "        else:\n",
    "            bad += 1\n",
    "        print(f\"[{ep}/{epochs}] loss {tr_loss:.4f} | {m}\")\n",
    "        if bad>=patience:\n",
    "            print(\"Early stop.\"); break\n",
    "    return f\"{out_dir}/best.pt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3e-6hlv2O5v"
   },
   "source": [
    "### Threshold para clasificación binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pick_threshold(model, loader):\n",
    "    model.eval()\n",
    "    probs=[]; gts=[]\n",
    "    for x, yb, _, _ in loader:\n",
    "        x = x.to(device)\n",
    "        logit, _ = model(x)\n",
    "        probs.append(torch.sigmoid(logit).cpu().numpy())\n",
    "        gts.append(yb.numpy())\n",
    "    p = np.concatenate(probs); y = np.concatenate(gts).astype(int)\n",
    "    best_f1, best_t = -1, 0.5\n",
    "    for t in np.linspace(0.2,0.8,25):\n",
    "        f1 = f1_score((p>=t).astype(int), y)\n",
    "        if f1>best_f1: best_f1, best_t = f1, t\n",
    "    return best_t, best_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tzBYVLe2mJ4"
   },
   "source": [
    "### Interpretación de Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gradcam_overlay(model, x_tensor, target_layer=None):\n",
    "    model.eval()\n",
    "    if target_layer is None:\n",
    "        # capa final del backbone\n",
    "        target_layer = [model.backbone.get_global_pool().flatten]\n",
    "        # fallback: usa la última conv si el backbone lo expone\n",
    "        target_layer = [list(model.backbone.modules())[-2]]\n",
    "    cam = GradCAM(model=model, target_layers=target_layer, use_cuda=(device.type==\"cuda\"))\n",
    "    targets = [BinaryClassifierOutputTarget(1)]  # clase \"con barra\"\n",
    "    grayscale_cam = cam(input_tensor=x_tensor, targets=targets)[0]\n",
    "    img = x_tensor[0].permute(1,2,0).cpu().numpy()\n",
    "    img = (img - img.min())/(img.max()-img.min()+1e-8)\n",
    "    vis = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
    "    return vis  # array RGB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1lrG5Ex21tX"
   },
   "source": [
    "## Ejecución de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybDOj5nz24dE"
   },
   "source": [
    "- Tamaño de las imágenes (`256x256`).\n",
    "- Número de canales de entrada (`3`).\n",
    "- Número de épocas (`30`).\n",
    "- Tasa de aprendizaje (`2e-4`).\n",
    "- Batch size (`32`).\n",
    "- Backbone (`tf_efficientnet_b0_ns`).\n",
    "- Directorio de salida para checkpoints y logs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = \"/content/drive/MyDrive/Proyecto_Integrador/Clasificacion_Presencia_Barra/RNN_EfficientNet-B0_Diego/datasets\"\n",
    "ckpt = fit_paths(\n",
    "    csv_train=f\"{DATASETS_DIR}/train_grz.csv\",\n",
    "    csv_val  =f\"{DATASETS_DIR}/val_grz.csv\",\n",
    "    size=256,\n",
    "    in_chans=3,\n",
    "    epochs=30,\n",
    "    lr=2e-4,\n",
    "    bs=32,\n",
    "    backbone=\"tf_efficientnet_b0_ns\",\n",
    "    out_dir=\"/content/drive/MyDrive/Proyecto_Integrador/Clasificacion_Presencia_Barra/RNN_EfficientNet-B0_Diego/runs/barnet\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9zSz5GQ3Wlu"
   },
   "source": [
    "## Evaluación final del modelo BarNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBPYUt0r3cIQ"
   },
   "source": [
    "Conjunto de test con métricas de clasificación, regresión y análisis de errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar modelo con weights_only=False\n",
    "DATASETS_DIR = \"/content/drive/MyDrive/Proyecto_Integrador/Clasificacion_Presencia_Barra/RNN_EfficientNet-B0_Diego/datasets\"\n",
    "BEST = \"/content/drive/MyDrive/Proyecto_Integrador/Clasificacion_Presencia_Barra/RNN_EfficientNet-B0_Diego/runs/barnet/best.pt\"\n",
    "\n",
    "# Cargar checkpoint\n",
    "ckpt = torch.load(BEST, map_location=device, weights_only=False)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Modelo cargado exitosamente\")\n",
    "\n",
    "# 2. Preparar test loader\n",
    "TEST_CSV = f\"{DATASETS_DIR}/test_grz.csv\"\n",
    "test_ds = BarsDatasetPaths(TEST_CSV, size=256, train=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=64, num_workers=2, pin_memory=True)\n",
    "\n",
    "# 3. Predecir\n",
    "probs = []\n",
    "y_true = []\n",
    "strengths_pred = []\n",
    "strengths_true = []\n",
    "image_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, yb, ys, img_id in test_dl:\n",
    "        x = x.to(device)\n",
    "        logits, sp = model(x)\n",
    "\n",
    "        probs.append(torch.sigmoid(logits).cpu().numpy())\n",
    "        y_true.append(yb.numpy())\n",
    "        strengths_pred.append(torch.sigmoid(sp).cpu().numpy())\n",
    "        strengths_true.append(ys.numpy())\n",
    "        image_ids.extend(img_id)\n",
    "\n",
    "probs = np.concatenate(probs)\n",
    "y_true = np.concatenate(y_true).astype(int)\n",
    "y_pred = (probs >= 0.5).astype(int)\n",
    "strengths_pred = np.concatenate(strengths_pred)\n",
    "strengths_true = np.concatenate(strengths_true)\n",
    "\n",
    "# 4. METRICAS FINALES\n",
    "print(\"=\"*60)\n",
    "print(\"           RESULTADOS EN TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clasificacion\n",
    "print(\"\\nReporte de Clasificacion:\")\n",
    "print(classification_report(y_true, y_pred,\n",
    "                          target_names=[\"Sin barra (0)\", \"Con barra (1)\"],\n",
    "                          digits=4))\n",
    "\n",
    "# Matriz de confusion\n",
    "print(\"\\nMatriz de Confusion:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(f\"                Pred: Sin barra | Con barra\")\n",
    "print(f\"Real: Sin barra       {cm[0,0]:4d}        {cm[0,1]:4d}\")\n",
    "print(f\"      Con barra       {cm[1,0]:4d}        {cm[1,1]:4d}\")\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nMetricas Agregadas:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  AUPRC:     {average_precision_score(y_true, probs):.4f}\")\n",
    "print(f\"  AUROC:     {roc_auc_score(y_true, probs):.4f}\")\n",
    "\n",
    "# Error en fuerza\n",
    "mae_strength = np.mean(np.abs(strengths_pred - strengths_true))\n",
    "print(f\"  MAE (fuerza): {mae_strength:.4f}\")\n",
    "\n",
    "# 5. Guardar predicciones\n",
    "results = pd.DataFrame({\n",
    "    'image_id': image_ids,\n",
    "    'true_label': y_true,\n",
    "    'pred_prob': probs,\n",
    "    'pred_label': y_pred,\n",
    "    'true_strength': strengths_true,\n",
    "    'pred_strength': strengths_pred,\n",
    "    'correct': y_true == y_pred,\n",
    "    'error': np.abs(y_true - y_pred)\n",
    "})\n",
    "\n",
    "OUT_CSV = f\"{DATASETS_DIR}/test_results.csv\"\n",
    "results.to_csv(OUT_CSV, index=False)\n",
    "print(f\"\\nGuardado: {OUT_CSV}\")\n",
    "\n",
    "# 6. Analisis de errores\n",
    "errors = results[~results['correct']]\n",
    "print(f\"\\nErrores: {len(errors)}/{len(results)} ({100*len(errors)/len(results):.2f}%)\")\n",
    "\n",
    "fp_errors = errors[errors['true_label'] == 0]\n",
    "print(f\"  Falsos Positivos: {len(fp_errors)}\")\n",
    "\n",
    "fn_errors = errors[errors['true_label'] == 1]\n",
    "print(f\"  Falsos Negativos: {len(fn_errors)}\")\n",
    "\n",
    "print(\"\\nEvaluacion completa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSsg3D99E8H7"
   },
   "source": [
    "# Modelo 2: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPFSWuifFfeZ"
   },
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znAnxFH2Fgg8"
   },
   "source": [
    "### Instalación y google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost scikit-learn scikit-image scipy matplotlib seaborn shap -q\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EikLSy1JFkVx"
   },
   "source": [
    "### Inputs y Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    average_precision_score, roc_auc_score, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from astropy.io import fits\n",
    "import cv2\n",
    "\n",
    "# Configuración\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Rutas del proyecto\n",
    "PROJ_ROOT = \"/content/drive/MyDrive/Proyecto_Integrador\"\n",
    "DATASETS_DIR = f\"{PROJ_ROOT}/Clasificacion_Presencia_Barra/RNN_EfficientNet-B0_Diego/datasets\"\n",
    "XGB_DIR = f\"{PROJ_ROOT}/Clasificacion_Presencia_Barra/XGBoost_Diego\"\n",
    "os.makedirs(XGB_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Dispositivo: {device}\")\n",
    "print(f\"Directorio XGBoost: {XGB_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjkJjuuGFqew"
   },
   "source": [
    "## Procesamiento de Imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_fits_first2d(path):\n",
    "    with fits.open(path, memmap=False) as hdul:\n",
    "        for h in hdul:\n",
    "            if h.data is not None and getattr(h.data, \"ndim\", 0) == 2:\n",
    "                arr = h.data.astype(np.float32)\n",
    "                break\n",
    "    return np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def _read_any_gray(path):\n",
    "    p = str(path).lower()\n",
    "    if p.endswith(\".fits\") or p.endswith(\".fz\"):\n",
    "        return _read_fits_first2d(path)\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(path)\n",
    "    if img.ndim == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "def _stretch_asinh(x):\n",
    "    lo, hi = np.percentile(x, [1, 99.5])\n",
    "    x = np.clip((x - lo) / (hi - lo + 1e-6), 0, 1)\n",
    "    return np.arcsinh(10 * x) / np.arcsinh(10)\n",
    "\n",
    "def _pad_resize_square(img, size=256):\n",
    "    h, w = img.shape\n",
    "    m = max(h, w)\n",
    "    canvas = np.zeros((m, m), np.float32)\n",
    "    y0 = (m - h) // 2\n",
    "    x0 = (m - w) // 2\n",
    "    canvas[y0:y0+h, x0:x0+w] = img\n",
    "    return cv2.resize(canvas, (size, size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def stack_from_row(row, size=256):\n",
    "    chans = []\n",
    "    for b in [\"g\", \"r\", \"z\"]:\n",
    "        p = row[f\"path_{b}\"]\n",
    "        img = _read_any_gray(p)\n",
    "        img = _pad_resize_square(_stretch_asinh(img), size)\n",
    "        chans.append(img)\n",
    "    return np.stack(chans, axis=0)  # CxHxW\n",
    "\n",
    "print(\"✓ Funciones de carga de imágenes definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZupWqkDsFvnK"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statistical_features(img_3ch):\n",
    "    \"\"\"\n",
    "    Extrae features estadísticos de una imagen de 3 canales (g, r, z)\n",
    "    ESTE ES EL ÚNICO ENFOQUE - sin usar redes neuronales\n",
    "\n",
    "    Args:\n",
    "        img_3ch: numpy array shape (3, H, W)\n",
    "\n",
    "    Returns:\n",
    "        dict con features estadísticos tradicionales\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # FEATURES POR BANDA (g, r, z)\n",
    "    for i, band in enumerate(['g', 'r', 'z']):\n",
    "        channel = img_3ch[i]\n",
    "\n",
    "        # Estadísticos básicos de distribución\n",
    "        features[f'{band}_mean'] = np.mean(channel)\n",
    "        features[f'{band}_std'] = np.std(channel)\n",
    "        features[f'{band}_median'] = np.median(channel)\n",
    "        features[f'{band}_min'] = np.min(channel)\n",
    "        features[f'{band}_max'] = np.max(channel)\n",
    "        features[f'{band}_q25'] = np.percentile(channel, 25)\n",
    "        features[f'{band}_q75'] = np.percentile(channel, 75)\n",
    "        features[f'{band}_skew'] = float(pd.Series(channel.flatten()).skew())\n",
    "        features[f'{band}_kurt'] = float(pd.Series(channel.flatten()).kurt())\n",
    "\n",
    "        # Contraste y rango dinámico\n",
    "        features[f'{band}_contrast'] = np.max(channel) - np.min(channel)\n",
    "        features[f'{band}_range'] = np.percentile(channel, 95) - np.percentile(channel, 5)\n",
    "\n",
    "        # Entropía (complejidad de la imagen)\n",
    "        hist, _ = np.histogram(channel, bins=256, range=(0, 1))\n",
    "        hist = hist / (hist.sum() + 1e-8)\n",
    "        features[f'{band}_entropy'] = -np.sum(hist * np.log2(hist + 1e-8))\n",
    "\n",
    "        # Momentos espaciales (detectan asimetría)\n",
    "        Y, X = np.mgrid[0:channel.shape[0], 0:channel.shape[1]]\n",
    "        total_mass = channel.sum() + 1e-8\n",
    "        features[f'{band}_centroid_x'] = (channel * X).sum() / total_mass\n",
    "        features[f'{band}_centroid_y'] = (channel * Y).sum() / total_mass\n",
    "\n",
    "        # Gradientes (detectan bordes y estructuras)\n",
    "        gy, gx = np.gradient(channel)\n",
    "        grad_mag = np.sqrt(gx**2 + gy**2)\n",
    "        features[f'{band}_grad_mean'] = np.mean(grad_mag)\n",
    "        features[f'{band}_grad_std'] = np.std(grad_mag)\n",
    "        features[f'{band}_grad_max'] = np.max(grad_mag)\n",
    "\n",
    "        # Textura (varianza local)\n",
    "        from scipy.ndimage import uniform_filter\n",
    "        local_mean = uniform_filter(channel, size=5)\n",
    "        local_var = uniform_filter(channel**2, size=5) - local_mean**2\n",
    "        features[f'{band}_texture_var'] = np.mean(local_var)\n",
    "\n",
    "    # FEATURES ENTRE BANDAS (información de color)\n",
    "    features['gr_ratio'] = np.mean(img_3ch[0]) / (np.mean(img_3ch[1]) + 1e-8)\n",
    "    features['rz_ratio'] = np.mean(img_3ch[1]) / (np.mean(img_3ch[2]) + 1e-8)\n",
    "    features['gz_ratio'] = np.mean(img_3ch[0]) / (np.mean(img_3ch[2]) + 1e-8)\n",
    "\n",
    "    # Índices de color astronómicos\n",
    "    features['color_gr'] = -2.5 * np.log10(features['gr_ratio'] + 1e-8)\n",
    "    features['color_rz'] = -2.5 * np.log10(features['rz_ratio'] + 1e-8)\n",
    "\n",
    "    # Correlación entre bandas\n",
    "    features['corr_gr'] = np.corrcoef(img_3ch[0].flatten(), img_3ch[1].flatten())[0,1]\n",
    "    features['corr_rz'] = np.corrcoef(img_3ch[1].flatten(), img_3ch[2].flatten())[0,1]\n",
    "\n",
    "    return features\n",
    "\n",
    "# Test\n",
    "df_test = pd.read_csv(f\"{DATASETS_DIR}/train_grz.csv\").head(1)\n",
    "img_test = stack_from_row(df_test.iloc[0], size=256)\n",
    "feats_test = extract_statistical_features(img_test)\n",
    "print(f\"✓ Features estadísticos extraídos: {len(feats_test)} features\")\n",
    "print(\"Ejemplos:\", list(feats_test.keys())[:10])\n",
    "print(\"\\n⚠️  MODO: XGBoost PURO (sin embeddings de redes neuronales)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(csv_path, max_samples=None):\n",
    "    \"\"\"\n",
    "    Extrae SOLO features estadísticos de todas las galaxias\n",
    "    NO usa redes neuronales - enfoque tradicional de machine learning\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if max_samples:\n",
    "        df = df.head(max_samples)\n",
    "\n",
    "    print(f\"Extrayendo features estadísticos de {len(df)} muestras...\")\n",
    "    print(\"  Método: Features tradicionales (sin deep learning)\")\n",
    "\n",
    "    all_features = []\n",
    "    y_bin = []\n",
    "    y_str = []\n",
    "    ids = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            img = stack_from_row(row, size=256)\n",
    "            feat_dict = extract_statistical_features(img)\n",
    "\n",
    "            all_features.append(feat_dict)\n",
    "            y_bin.append(row['label_bin'])\n",
    "            y_str.append(row['Bars'])\n",
    "            ids.append(row['image_id'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en {row['image_id']}: {e}\")\n",
    "            continue\n",
    "\n",
    "    X_df = pd.DataFrame(all_features)\n",
    "    X = X_df.values\n",
    "\n",
    "    print(f\"✓ Features extraídos: shape {X.shape}\")\n",
    "    print(f\"  Total features estadísticos: {X.shape[1]}\")\n",
    "\n",
    "    return X, np.array(y_bin), np.array(y_str), ids, list(X_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXTRAYENDO FEATURES DE TODOS LOS SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n[1/3] TRAIN SET\")\n",
    "X_train, y_train, ys_train, ids_train, feature_names = extract_all_features(\n",
    "    f\"{DATASETS_DIR}/train_grz.csv\"\n",
    ")\n",
    "\n",
    "print(\"\\n[2/3] VALIDATION SET\")\n",
    "X_val, y_val, ys_val, ids_val, _ = extract_all_features(\n",
    "    f\"{DATASETS_DIR}/val_grz.csv\"\n",
    ")\n",
    "\n",
    "print(\"\\n[3/3] TEST SET\")\n",
    "X_test, y_test, ys_test, ids_test, _ = extract_all_features(\n",
    "    f\"{DATASETS_DIR}/test_grz.csv\"\n",
    ")\n",
    "\n",
    "# Guardar features\n",
    "np.savez(\n",
    "    f\"{XGB_DIR}/features.npz\",\n",
    "    X_train=X_train, y_train=y_train, ys_train=ys_train,\n",
    "    X_val=X_val, y_val=y_val, ys_val=ys_val,\n",
    "    X_test=X_test, y_test=y_test, ys_test=ys_test,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Features guardados en: {XGB_DIR}/features.npz\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Val:   {X_val.shape}\")\n",
    "print(f\"  Test:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyty0zD7F98A"
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENTRENAMIENTO XGBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['auc', 'aucpr', 'logloss'],\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist' if device.type == 'cuda' else 'hist',\n",
    "    'early_stopping_rounds': 20,  # ← MOVIDO AQUÍ\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(**params)\n",
    "\n",
    "print(\"\\nParámetros del modelo:\")\n",
    "for k, v in params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nEntrenando...\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "model_path = f\"{XGB_DIR}/xgb_model.json\"\n",
    "model.save_model(model_path)\n",
    "print(f\"\\n✓ Modelo guardado: {model_path}\")\n",
    "print(f\"✓ Mejor iteración: {model.best_iteration}\")\n",
    "print(f\"✓ Mejor score validación: {model.best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_Csz6EjGAV4"
   },
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred = (y_val_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUACIÓN EN VALIDACIÓN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "acc = accuracy_score(y_val, y_val_pred)\n",
    "prec = precision_score(y_val, y_val_pred)\n",
    "rec = recall_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "auprc = average_precision_score(y_val, y_val_pred_proba)\n",
    "auroc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "\n",
    "print(f\"\\nMétricas (threshold=0.5):\")\n",
    "print(f\"  Accuracy:  {acc:.4f}\")\n",
    "print(f\"  Precision: {prec:.4f}\")\n",
    "print(f\"  Recall:    {rec:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  AUPRC:     {auprc:.4f}\")\n",
    "print(f\"  AUROC:     {auroc:.4f}\")\n",
    "\n",
    "# Optimizar threshold\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_val_pred_proba)\n",
    "f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-8)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"\\n✓ Mejor threshold: {best_threshold:.3f}\")\n",
    "print(f\"✓ F1-Score óptimo: {f1_scores[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJKOo2aZGERs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUACIÓN FINAL EN TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred = (y_test_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_auprc = average_precision_score(y_test, y_test_pred_proba)\n",
    "test_auroc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "print(f\"\\nMétricas finales (threshold={best_threshold:.3f}):\")\n",
    "print(f\"  Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"  Precision: {test_prec:.4f}\")\n",
    "print(f\"  Recall:    {test_rec:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "print(f\"  AUPRC:     {test_auprc:.4f}\")\n",
    "print(f\"  AUROC:     {test_auroc:.4f}\")\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "print(f\"\\nMatriz de Confusión:\")\n",
    "print(f\"                Pred: Sin barra | Con barra\")\n",
    "print(f\"Real: Sin barra      {cm_test[0,0]:4d}         {cm_test[0,1]:4d}\")\n",
    "print(f\"      Con barra      {cm_test[1,0]:4d}         {cm_test[1,1]:4d}\")\n",
    "\n",
    "# Guardar resultados\n",
    "results_df = pd.DataFrame({\n",
    "    'image_id': ids_test,\n",
    "    'true_label': y_test,\n",
    "    'pred_prob': y_test_pred_proba,\n",
    "    'pred_label': y_test_pred,\n",
    "    'true_strength': ys_test,\n",
    "    'correct': y_test == y_test_pred\n",
    "})\n",
    "results_df.to_csv(f\"{XGB_DIR}/test_results.csv\", index=False)\n",
    "print(f\"\\n✓ Resultados guardados: {XGB_DIR}/test_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRbqq1EdGFj4"
   },
   "source": [
    "### Importancia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"IMPORTANCIA DE FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "importance = model.feature_importances_\n",
    "feat_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 features más importantes:\")\n",
    "print(feat_importance.head(20).to_string(index=False))\n",
    "\n",
    "feat_importance.to_csv(f\"{XGB_DIR}/feature_importance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FmJ73gFGH9f"
   },
   "source": [
    "## Guardar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package = {\n",
    "    'model': model,\n",
    "    'feature_names': feature_names,\n",
    "    'best_threshold': best_threshold,\n",
    "    'metrics': {\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_f1': float(test_f1),\n",
    "        'test_auprc': float(test_auprc),\n",
    "        'test_auroc': float(test_auroc)\n",
    "    }\n",
    "}\n",
    "\n",
    "pickle_path = f\"{XGB_DIR}/xgb_complete.pkl\"\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(f\"✓ Modelo completo guardado: {pickle_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✨ ENTRENAMIENTO COMPLETADO ✨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63kHbpCFGzVg"
   },
   "source": [
    "# Modelo 3: CatBoost (Tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = \"/content/drive/MyDrive/Proyecto_Integrador/data/dataset.csv\"\n",
    "DST = \"/content/dataset_local.csv\"\n",
    "\n",
    "import os, shutil, time\n",
    "\n",
    "assert os.path.exists(SRC), f\"No existe el archivo en Drive: {SRC}\"\n",
    "\n",
    "last_err = None\n",
    "for attempt in range(3):\n",
    "    try:\n",
    "        shutil.copy2(SRC, DST)\n",
    "        print(f\"Copiado a {DST} (intento {attempt+1})\")\n",
    "        last_err = None\n",
    "        break\n",
    "    except OSError as e:\n",
    "        last_err = e\n",
    "        print(f\"Falló la copia (intento {attempt+1}): {e}\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "if last_err is not None:\n",
    "    raise last_err\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(DST, encoding=\"utf-8\", engine=\"python\")\n",
    "\n",
    "df.columns = df.columns.str.strip().str.replace(r\"\\s+\", \"_\", regex=True).str.lower()\n",
    "\n",
    "rename_map = {\n",
    "    \"bars\": \"target\",\n",
    "    \"name\": \"id\",\n",
    "}\n",
    "df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "\n",
    "BARS_POSITIVE_THRESHOLD = 0.0\n",
    "assert \"target\" in df.columns, \"No se encontró columna Bars/target en el CSV\"\n",
    "df[\"target\"] = (df[\"target\"] > BARS_POSITIVE_THRESHOLD).astype(int)\n",
    "\n",
    "TARGET_COL = \"target\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "print(\"Columnas:\", list(df.columns))\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Distribución de target:\\n\", df[TARGET_COL].value_counts(dropna=False))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkg):\n",
    "    try:\n",
    "        __import__(pkg.split(\"==\")[0].split(\">=\")[0].split(\"[\")[0])\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "pip_install(\"pandas\")\n",
    "pip_install(\"numpy\")\n",
    "pip_install(\"scikit-learn\")\n",
    "pip_install(\"catboost\")\n",
    "pip_install(\"joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, balanced_accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from joblib import dump\n",
    "\n",
    "artifacts_dir = Path(\"artifacts\"); artifacts_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_KM_S = 300_000\n",
    "H0 = 70.0\n",
    "ARCSEC_PER_RAD = 206265.0\n",
    "\n",
    "def mpc_from_z(z):\n",
    "    return (C_KM_S * z) / H0\n",
    "\n",
    "def kpc_per_arcsec_from_z(z):\n",
    "    d_mpc = mpc_from_z(z)\n",
    "    return (d_mpc / ARCSEC_PER_RAD) * 1000.0\n",
    "\n",
    "def compute_lengths(df, pixel_col=\"pixels_len\", z_col=\"z\", arcsec_col=\"arcsec_len\", kpc_col=\"kpc_len\"):\n",
    "    out = df.copy()\n",
    "    if pixel_col in out.columns and z_col in out.columns:\n",
    "        out[arcsec_col] = out[pixel_col] * PIXEL_SCALE_ARCSEC\n",
    "        out[kpc_col] = out.apply(lambda r: r[arcsec_col] * kpc_per_arcsec_from_z(r[z_col]) if pd.notnull(r[z_col]) else np.nan, axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FEATURES_CSV)\n",
    "print(\"Cols:\", list(df.columns))\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "for candidate_pixels, candidate_z in [(\"pixels_len\",\"z\"), (\"bar_pixels\",\"z\")]:\n",
    "    if candidate_pixels in df.columns and candidate_z in df.columns:\n",
    "        df = compute_lengths(df, pixel_col=candidate_pixels, z_col=candidate_z)\n",
    "        break\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip().str.replace(r\"\\s+\", \"_\", regex=True).str.lower()\n",
    "\n",
    "rename_map = {\n",
    "    \"name\": \"id\",\n",
    "    \"bars\": \"target\"\n",
    "}\n",
    "df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "\n",
    "if \"target\" not in df.columns:\n",
    "    raise ValueError(f\"No se encontró la columna 'Bars' o 'target' en el CSV. Columnas detectadas: {list(df.columns)}\")\n",
    "\n",
    "BARS_POSITIVE_THRESHOLD = 0.0\n",
    "df[\"target\"] = (df[\"target\"] > BARS_POSITIVE_THRESHOLD).astype(int)\n",
    "\n",
    "print(\"Columnas después de renombrar:\", list(df.columns))\n",
    "print(\"Distribución de target:\", df[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TARGET_COL in df.columns, f\"No se encontró la columna objetivo '{TARGET_COL}'\"\n",
    "y = df[TARGET_COL].astype(int).values\n",
    "\n",
    "drop_cols = [TARGET_COL]\n",
    "if \"id\" in df.columns: drop_cols.append(\"id\")\n",
    "X = df.drop(columns=drop_cols)\n",
    "\n",
    "cat_cols = [c for c in X.columns if X[c].dtype == 'object' or X[c].dtype.name.startswith(\"category\")]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Numéricas:\", len(num_cols), \"| Categóricas:\", len(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(X_train, y_train, cat_features=[X_train.columns.get_loc(c) for c in cat_cols])\n",
    "valid_pool = Pool(X_test, y_test, cat_features=[X_test.columns.get_loc(c) for c in cat_cols])\n",
    "\n",
    "candidates = [\n",
    "    dict(depth=6, learning_rate=0.05, l2_leaf_reg=3, border_count=128),\n",
    "    dict(depth=8, learning_rate=0.03, l2_leaf_reg=5, border_count=254),\n",
    "    dict(depth=10, learning_rate=0.03, l2_leaf_reg=8, border_count=254),\n",
    "    dict(depth=6, learning_rate=0.1, l2_leaf_reg=3, border_count=128),\n",
    "]\n",
    "\n",
    "best_model, best_f1 = None, -1.0\n",
    "history = []\n",
    "\n",
    "for i, params in enumerate(candidates, 1):\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=5000,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        depth=params[\"depth\"],\n",
    "        l2_leaf_reg=params[\"l2_leaf_reg\"],\n",
    "        border_count=params[\"border_count\"],\n",
    "        loss_function=\"Logloss\",\n",
    "        eval_metric=\"F1\",\n",
    "        random_seed=RANDOM_STATE,\n",
    "        class_weights=class_weights,\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=200, verbose=False)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    preds = (proba >= 0.5).astype(int)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    bacc = balanced_accuracy_score(y_test, preds)\n",
    "    history.append(dict(run=i, params=params, f1=f1, auc=auc, acc=acc, bacc=bacc))\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = model\n",
    "\n",
    "pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = best_model.predict_proba(X_test)[:, 1]\n",
    "preds = (proba >= 0.5).astype(int)\n",
    "print(classification_report(y_test, preds, digits=4))\n",
    "print(\"F1:\", round(f1_score(y_test, preds), 4))\n",
    "print(\"ROC-AUC:\", round(roc_auc_score(y_test, proba), 4))\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, preds), 4))\n",
    "print(\"Balanced Acc:\", round(balanced_accuracy_score(y_test, preds), 4))\n",
    "\n",
    "model_path = artifacts_dir / \"catboost_tabular.cbm\"\n",
    "best_model.save_model(str(model_path))\n",
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2CzyqFkG1AR"
   },
   "source": [
    "# Modelo 4: **SVM (RBF) con HOG + PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys, subprocess\n",
    "def pip_install(pkg):\n",
    "    try:\n",
    "        __import__(pkg.split('==')[0].split('>=')[0].split('[')[0])\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "\n",
    "for pkg in [\n",
    "    'pandas','numpy','matplotlib','scikit-learn','scikit-image','joblib','tqdm','astropy'\n",
    "]:\n",
    "    pip_install(pkg)\n",
    "\n",
    "from pathlib import Path\n",
    "ROOT_DRIVE = Path('/content/drive/MyDrive')\n",
    "IMAGE_DIR = ROOT_DRIVE / 'Proyecto_Integrador/data/images'\n",
    "LABELS_CSV = str(ROOT_DRIVE / 'Proyecto_Integrador/data/dataset.csv')\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "N_JOBS = -1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "HOG_PARAMS = dict(\n",
    "    orientations=9,\n",
    "    pixels_per_cell=(16, 16),\n",
    "    cells_per_block=(2, 2),\n",
    "    block_norm='L2-Hys',\n",
    "    transform_sqrt=True,\n",
    "    feature_vector=True,\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "artifacts_dir = Path('artifacts'); artifacts_dir.mkdir(exist_ok=True)\n",
    "print('Artifacts dir:', artifacts_dir.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from astropy.io import fits\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from joblib import dump\n",
    "\n",
    "FLEX_PAT = re.compile(r'^(?P<prefix>.+)_(?P<band>[ugrizy])\\.(fits|fit)(\\.fz)?$', re.IGNORECASE)\n",
    "\n",
    "def reindex_fits_map(image_dir: Path):\n",
    "    fits_map = {}\n",
    "    band_counts = Counter()\n",
    "    for p in glob.glob(str(image_dir / '**/*'), recursive=True):\n",
    "        pp = Path(p)\n",
    "        if not pp.is_file():\n",
    "            continue\n",
    "        m = FLEX_PAT.match(pp.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        prefix = m.group('prefix').lower()\n",
    "        band = m.group('band').lower()\n",
    "        band_counts[band] += 1\n",
    "        fits_map.setdefault(prefix, {})[band] = str(pp)\n",
    "    return fits_map, band_counts\n",
    "\n",
    "def read_fits_band(path: str):\n",
    "    try:\n",
    "        with fits.open(path, memmap=False) as hdul:\n",
    "            data = hdul[0].data\n",
    "            if data is None:\n",
    "                return None\n",
    "            data = np.array(data, dtype=np.float32)\n",
    "            data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            vmin, vmax = np.percentile(data, [0.5, 99.5])\n",
    "            scale = (vmax - vmin) if (vmax > vmin) else 1.0\n",
    "            data = np.clip((data - vmin) / (scale + 1e-8), 0, 1)\n",
    "            return data\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_image_from_bands(prefix: str, fits_map: dict):\n",
    "    entry = fits_map.get(prefix.lower(), {})\n",
    "    if not entry:\n",
    "        return None, []\n",
    "    for (R, G, B) in [('r','g','i'), ('r','g','z'), ('i','r','g'), ('z','r','g'), ('r','i','z'), ('g','i','z')]:\n",
    "        if all(b in entry for b in (R, G, B)):\n",
    "            r = read_fits_band(entry[R]); g = read_fits_band(entry[G]); b = read_fits_band(entry[B])\n",
    "            if any(x is None for x in (r, g, b)):\n",
    "                continue\n",
    "            return np.stack([r, g, b], axis=-1), [R, G, B]\n",
    "    for (A, B) in [('r','g'), ('g','z'), ('r','z'), ('g','i'), ('r','i'), ('i','z')]:\n",
    "        if A in entry and B in entry:\n",
    "            a = read_fits_band(entry[A]); b = read_fits_band(entry[B])\n",
    "            if any(x is None for x in (a, b)):\n",
    "                continue\n",
    "            return np.stack([a, b, (a + b) / 2.0], axis=-1), [A, B]\n",
    "    for b in ('r','g','i','z','u','y'):\n",
    "        if b in entry:\n",
    "            a = read_fits_band(entry[b])\n",
    "            if a is None:\n",
    "                continue\n",
    "            return np.stack([a, a, a], axis=-1), [b]\n",
    "    return None, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all = pd.read_csv(LABELS_CSV)\n",
    "labels_all.columns = labels_all.columns.str.strip().str.lower()\n",
    "assert 'name' in labels_all.columns and 'bars' in labels_all.columns, labels_all.columns\n",
    "labels_all['label'] = (labels_all['bars'] > 0).astype(int)\n",
    "labels_all['prefix'] = labels_all['name'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "fits_map, band_counts = reindex_fits_map(IMAGE_DIR)\n",
    "print('Conteo por banda (inicio):', dict(band_counts))\n",
    "print('Prefixes indexados (inicio):', len(fits_map))\n",
    "\n",
    "def is_buildable(prefix: str) -> bool:\n",
    "    rgb, used = build_image_from_bands(prefix, fits_map)\n",
    "    return rgb is not None\n",
    "\n",
    "labels_all['is_buildable'] = [\n",
    "    is_buildable(pref) for pref in tqdm(labels_all['prefix'].tolist(), desc='Auditando construibles')\n",
    "]\n",
    "\n",
    "print('Disponibilidad por clase:', labels_all.groupby(['label','is_buildable']).size().unstack(fill_value=0).to_dict())\n",
    "\n",
    "missing_by_label = {\n",
    "    0: labels_all[(labels_all['label']==0) & (~labels_all['is_buildable'])]['prefix'].unique().tolist(),\n",
    "    1: labels_all[(labels_all['label']==1) & (~labels_all['is_buildable'])]['prefix'].unique().tolist(),\n",
    "}\n",
    "pd.Series(missing_by_label[0]).to_csv(artifacts_dir / 'missing_negatives_prefixes.csv', index=False, header=['prefix'])\n",
    "pd.Series(missing_by_label[1]).to_csv(artifacts_dir / 'missing_positives_prefixes.csv', index=False, header=['prefix'])\n",
    "print(f\"Negativos SIN FITS: {len(missing_by_label[0])} → artifacts/missing_negatives_prefixes.csv\")\n",
    "print(f\"Positivos SIN FITS: {len(missing_by_label[1])} → artifacts/missing_positives_prefixes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all = pd.read_csv(LABELS_CSV)\n",
    "labels_all.columns = labels_all.columns.str.strip().str.lower()\n",
    "assert 'name' in labels_all.columns and 'bars' in labels_all.columns, labels_all.columns\n",
    "labels_all['label'] = (labels_all['bars'] > 0).astype(int)\n",
    "labels_all['prefix'] = labels_all['name'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "fits_map, band_counts = reindex_fits_map(IMAGE_DIR)\n",
    "print('Conteo por banda (inicio):', dict(band_counts))\n",
    "print('Prefixes indexados (inicio):', len(fits_map))\n",
    "\n",
    "def is_buildable(prefix: str) -> bool:\n",
    "    rgb, used = build_image_from_bands(prefix, fits_map)\n",
    "    return rgb is not None\n",
    "\n",
    "labels_all['is_buildable'] = [\n",
    "    is_buildable(pref) for pref in tqdm(labels_all['prefix'].tolist(), desc='Auditando construibles')\n",
    "]\n",
    "\n",
    "print('Disponibilidad por clase:', labels_all.groupby(['label','is_buildable']).size().unstack(fill_value=0).to_dict())\n",
    "\n",
    "missing_by_label = {\n",
    "    0: labels_all[(labels_all['label']==0) & (~labels_all['is_buildable'])]['prefix'].unique().tolist(),\n",
    "    1: labels_all[(labels_all['label']==1) & (~labels_all['is_buildable'])]['prefix'].unique().tolist(),\n",
    "}\n",
    "pd.Series(missing_by_label[0]).to_csv(artifacts_dir / 'missing_negatives_prefixes.csv', index=False, header=['prefix'])\n",
    "pd.Series(missing_by_label[1]).to_csv(artifacts_dir / 'missing_positives_prefixes.csv', index=False, header=['prefix'])\n",
    "print(f\"Negativos SIN FITS: {len(missing_by_label[0])} → artifacts/missing_negatives_prefixes.csv\")\n",
    "print(f\"Positivos SIN FITS: {len(missing_by_label[1])} → artifacts/missing_positives_prefixes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_csv = artifacts_dir / 'missing_negatives_prefixes.csv'\n",
    "if not missing_csv.exists():\n",
    "    raise FileNotFoundError('No existe artifacts/missing_negatives_prefixes.csv; ejecuta la celda 3 primero.')\n",
    "missing_neg = pd.read_csv(missing_csv, header=0).iloc[:,0].astype(str).tolist()\n",
    "\n",
    "LINK_DIR = IMAGE_DIR / 'linked_negatives'\n",
    "LINK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Indexando archivos .fit/.fits/fz en todo el Drive')\n",
    "candidates = []\n",
    "for ext in ('**/*.fits', '**/*.fit', '**/*.fits.fz', '**/*.fit.fz'):\n",
    "    candidates.extend(glob.glob(str(ROOT_DRIVE / ext), recursive=True))\n",
    "print('Candidatos escaneados:', len(candidates))\n",
    "\n",
    "def looks_like_band(fname: str, prefix: str) -> bool:\n",
    "    m = FLEX_PAT.match(fname)\n",
    "    return bool(m and m.group('prefix').lower() == prefix.lower())\n",
    "\n",
    "found_map = defaultdict(list)\n",
    "for p in tqdm(missing_neg, desc='Buscando negativos en Drive'):\n",
    "    p_low = p.lower()\n",
    "    for full in candidates:\n",
    "        name = Path(full).name\n",
    "        if looks_like_band(name, p_low):\n",
    "            found_map[p_low].append(full)\n",
    "\n",
    "print('Prefijos negativos con ≥1 banda localizada:', sum(1 for v in found_map.values() if len(v)>0), '/', len(missing_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_map, band_counts = reindex_fits_map(IMAGE_DIR)\n",
    "print('Conteo por banda (reindex):', dict(band_counts))\n",
    "print('Prefixes indexados (reindex):', len(fits_map))\n",
    "\n",
    "labels_all = pd.read_csv(LABELS_CSV)\n",
    "labels_all.columns = labels_all.columns.str.strip().str.lower()\n",
    "labels_all['label'] = (labels_all['bars'] > 0).astype(int)\n",
    "labels_all['prefix'] = labels_all['name'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "def is_buildable_with_map(prefix: str) -> bool:\n",
    "    rgb, used = build_image_from_bands(prefix, fits_map)\n",
    "    return rgb is not None\n",
    "\n",
    "buildable_set = {p for p in labels_all['prefix'].unique() if is_buildable_with_map(p)}\n",
    "labels_ok = labels_all[labels_all['prefix'].isin(buildable_set)].copy()\n",
    "print('Construibles por clase:', labels_ok['label'].value_counts().to_dict())\n",
    "\n",
    "counts = labels_ok['label'].value_counts()\n",
    "if len(counts) < 2:\n",
    "    print('Aún no hay FITS negativos construibles. Saltando a fallback One-Class SVM en la celda 6...')\n",
    "else:\n",
    "    min_class = int(counts.min())\n",
    "    labels_bal = pd.concat([\n",
    "        labels_ok[labels_ok['label']==0].sample(min_class, random_state=RANDOM_STATE, replace=False),\n",
    "        labels_ok[labels_ok['label']==1].sample(min_class, random_state=RANDOM_STATE, replace=False),\n",
    "    ], axis=0).sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    print('Balanceado (construibles) ->', labels_bal['label'].value_counts().to_dict())\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    for _, row in tqdm(labels_bal.iterrows(), total=len(labels_bal), desc='Extrayendo HOG'):\n",
    "        rgb, used = build_image_from_bands(row['prefix'], fits_map)\n",
    "        if rgb is None:\n",
    "            continue\n",
    "        gray = rgb2gray(rgb)\n",
    "        gray = resize(gray, IMG_SIZE, anti_aliasing=True, preserve_range=True)\n",
    "        feat = hog(gray, **HOG_PARAMS)\n",
    "        X_list.append(feat)\n",
    "        y_list.append(int(row['label']))\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        raise RuntimeError('No se pudo extraer HOG tras el balanceo.')\n",
    "\n",
    "    X = np.vstack(X_list).astype(np.float32)\n",
    "    y = np.array(y_list, dtype=int)\n",
    "    dump((X, y), artifacts_dir / 'hog_balanced_buildable.joblib')\n",
    "    print('X:', X.shape, '| dist:', pd.Series(y).value_counts().to_dict())\n",
    "    %store X\n",
    "    %store y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, balanced_accuracy_score, classification_report\n",
    "\n",
    "if 'X' in globals() and 'y' in globals() and len(np.unique(y)) == 2:\n",
    "    print('Entrenando **SVM binario**...')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler(with_mean=True)),\n",
    "        ('pca', PCA(n_components=0.95, svd_solver='full', random_state=RANDOM_STATE)),\n",
    "        ('svm', SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=RANDOM_STATE))\n",
    "    ])\n",
    "    param_distributions = {\n",
    "        'pca__n_components': [0.80, 0.90, 0.95, 0.99],\n",
    "        'svm__C': np.logspace(-1, 3, 12),\n",
    "        'svm__gamma': np.logspace(-4, 1, 12)\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=40,\n",
    "        scoring='f1',\n",
    "        cv=cv,\n",
    "        n_jobs=N_JOBS,\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        refit=True\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    print('Mejores hiperparámetros:', search.best_params_)\n",
    "    print('Mejor F1 (CV):', search.best_score_)\n",
    "    best = search.best_estimator_\n",
    "    proba = best.predict_proba(X_test)[:, 1]\n",
    "    preds = (proba >= 0.5).astype(int)\n",
    "    print(classification_report(y_test, preds, digits=4))\n",
    "    print('F1:', round(f1_score(y_test, preds), 4))\n",
    "    try:\n",
    "        print('ROC-AUC:', round(roc_auc_score(y_test, proba), 4))\n",
    "    except Exception as e:\n",
    "        print('ROC-AUC no disponible:', e)\n",
    "    print('Accuracy:', round(accuracy_score(y_test, preds), 4))\n",
    "    print('Balanced Acc:', round(balanced_accuracy_score(y_test, preds), 4))\n",
    "    model_path = artifacts_dir / 'svm_hog_pca.joblib'\n",
    "    dump(best, model_path)\n",
    "    print('Modelo guardado en:', model_path)\n",
    "else:\n",
    "    print('Entrenando **Fallback One-Class SVM** (sólo clase positiva disponible) ...')\n",
    "    labels_all = pd.read_csv(LABELS_CSV)\n",
    "    labels_all.columns = labels_all.columns.str.strip().str.lower()\n",
    "    labels_all['label'] = (labels_all['bars'] > 0).astype(int)\n",
    "    labels_all['prefix'] = labels_all['name'].apply(lambda x: Path(x).stem)\n",
    "    positives = labels_all[labels_all['label']==1]['prefix'].unique().tolist()\n",
    "    Xp = []\n",
    "    for p in tqdm(positives, desc='Extrayendo HOG (positivos)'):\n",
    "        rgb, used = build_image_from_bands(p, fits_map)\n",
    "        if rgb is None:\n",
    "            continue\n",
    "        gray = rgb2gray(rgb)\n",
    "        gray = resize(gray, IMG_SIZE, anti_aliasing=True, preserve_range=True)\n",
    "        feat = hog(gray, **HOG_PARAMS)\n",
    "        Xp.append(feat)\n",
    "    if len(Xp) == 0:\n",
    "        raise RuntimeError('No se pudo extraer HOG de positivos para One-Class SVM.')\n",
    "    Xp = np.vstack(Xp).astype(np.float32)\n",
    "    oc_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler(with_mean=True)),\n",
    "        ('ocsvm', OneClassSVM(kernel='rbf', nu=0.05, gamma='scale'))\n",
    "    ])\n",
    "    oc_pipe.fit(Xp)\n",
    "    pred = oc_pipe.predict(Xp)\n",
    "    print('Inliers detectados (en positivos):', int((pred==1).sum()), '/', len(pred))\n",
    "    model_path = artifacts_dir / 'oneclass_svm_positive_only.joblib'\n",
    "    dump(oc_pipe, model_path)\n",
    "    print('Modelo One-Class guardado en:', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kuu8TsrCG2X4"
   },
   "source": [
    "# Modelo 5: Linear regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjd98dlxEDz8"
   },
   "source": [
    "##Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, re, gc, math, pickle, warnings\n",
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "directory = \"/content/drive/MyDrive/Proyecto_Integrador/Clasificacion_Presencia_Barra/LN_Jonathan/Dataset\"\n",
    "IMAGES_ROOT = \"/content/drive/MyDrive/Proyecto_Integrador/data/images\"\n",
    "\n",
    "# Semilla\n",
    "SEED = 42\n",
    "\n",
    "print(\"Fecha de ejecución:\", datetime.now())\n",
    "print(\"directory:\", directory)\n",
    "print(\"IMAGES_ROOT:\", IMAGES_ROOT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_existing(*paths):\n",
    "    for p in paths:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def load_split_raw(base_dir: str, stem: str) -> pd.DataFrame:\n",
    "    p_parquet = os.path.join(base_dir, f\"{stem}.parquet\")\n",
    "    p_csv     = os.path.join(base_dir, f\"{stem}.csv\")\n",
    "    p = p_parquet if os.path.exists(p_parquet) else (p_csv if os.path.exists(p_csv) else None)\n",
    "    if p is None:\n",
    "        raise FileNotFoundError(f\"No encontré {stem}.parquet/.csv en {base_dir}\")\n",
    "    return pd.read_parquet(p) if p.endswith(\".parquet\") else pd.read_csv(p)\n",
    "\n",
    "def normalize_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"label_bin\" not in df.columns:\n",
    "        if \"label\" in df.columns:\n",
    "            df[\"label_bin\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).round().astype(int)\n",
    "        elif \"label_str\" in df.columns:\n",
    "            df[\"label_bin\"] = pd.to_numeric(df[\"label_str\"], errors=\"coerce\").fillna(0).round().astype(int)\n",
    "        elif \"Bars\" in df.columns:\n",
    "            v = df[\"Bars\"].astype(str).str.lower()\n",
    "            df[\"label_bin\"] = v.isin([\"1\",\"true\",\"yes\",\"si\",\"sí\",\"bar\",\"bars\"]).astype(int)\n",
    "        else:\n",
    "            raise ValueError(\"No encontré columna de etiqueta (label_bin/label/label_str/Bars).\")\n",
    "    return df\n",
    "\n",
    "print(\"Contenido del directorio base:\")\n",
    "for f in glob.glob(os.path.join(directory, \"*\")):\n",
    "    print(\"-\", os.path.basename(f))\n",
    "\n",
    "df_tr_raw = load_split_raw(directory, \"train\")\n",
    "df_va_raw = load_split_raw(directory, \"val\")\n",
    "df_te_raw = load_split_raw(directory, \"test\")\n",
    "\n",
    "df_tr_raw = normalize_labels(df_tr_raw)\n",
    "df_va_raw = normalize_labels(df_va_raw)\n",
    "df_te_raw = normalize_labels(df_te_raw)\n",
    "\n",
    "print(\"train shape:\", df_tr_raw.shape, \"| val shape:\", df_va_raw.shape, \"| test shape:\", df_te_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) Resolución de rutas multi-raíz y multi-extensión + filtrado a filas completas\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Lista de raíces candidatas\n",
    "IMAGE_ROOTS = [\n",
    "    \"/content/drive/MyDrive/Proyecto_Integrador/data/images\",\n",
    "    \"/content/drive/MyDrive/Proyecto_Integrador/data/\",\n",
    "]\n",
    "\n",
    "# 2) Extensiones candidatas (si mezclas .fits/.fz, probamos ambas)\n",
    "CAND_EXTS = [\".fits\", \".fz\"]\n",
    "\n",
    "def build_paths_multiroot_multiext(df_raw, roots, exts):\n",
    "    df = df_raw.copy()\n",
    "    df[\"image_id\"] = df.get(\"image_id\", df.get(\"name\")).astype(str)\n",
    "\n",
    "    # Para cada root+ext creamos 3 columnas candidatas (g,r,z)\n",
    "    cand_groups = []\n",
    "    idx = 0\n",
    "    for root in roots:\n",
    "        for ext in exts:\n",
    "            gcol = f\"g_{idx}\"\n",
    "            rcol = f\"r_{idx}\"\n",
    "            zcol = f\"z_{idx}\"\n",
    "            df[gcol] = df[\"image_id\"].map(lambda t: os.path.join(root, f\"{t}_g{ext}\"))\n",
    "            df[rcol] = df[\"image_id\"].map(lambda t: os.path.join(root, f\"{t}_r{ext}\"))\n",
    "            df[zcol] = df[\"image_id\"].map(lambda t: os.path.join(root, f\"{t}_z{ext}\"))\n",
    "            cand_groups.append((gcol, rcol, zcol, root, ext))\n",
    "            idx += 1\n",
    "\n",
    "    # Elige la PRIMERA combinación root+ext en la que existan las 3 bandas\n",
    "    def pick_first_existing(row):\n",
    "        for gcol, rcol, zcol, root, ext in cand_groups:\n",
    "            g, r, z = row[gcol], row[rcol], row[zcol]\n",
    "            if all(isinstance(p, str) and os.path.exists(p) for p in [g, r, z]):\n",
    "                return g, r, z, root, ext\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    chosen = df.apply(pick_first_existing, axis=1, result_type=\"expand\")\n",
    "    df[\"path_g\"] = chosen[0]\n",
    "    df[\"path_r\"] = chosen[1]\n",
    "    df[\"path_z\"] = chosen[2]\n",
    "    df[\"_chosen_root\"] = chosen[3]\n",
    "    df[\"_chosen_ext\"]  = chosen[4]\n",
    "\n",
    "    # Limpieza columnas temporales\n",
    "    drop_cols = []\n",
    "    for gcol, rcol, zcol, _, _ in cand_groups:\n",
    "        drop_cols.extend([gcol, rcol, zcol])\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_tr = build_paths_multiroot_multiext(df_tr_raw, IMAGE_ROOTS, CAND_EXTS)\n",
    "df_va = build_paths_multiroot_multiext(df_va_raw, IMAGE_ROOTS, CAND_EXTS)\n",
    "df_te = build_paths_multiroot_multiext(df_te_raw, IMAGE_ROOTS, CAND_EXTS)\n",
    "\n",
    "def count_missing(df, col):\n",
    "    return (~df[col].apply(lambda p: isinstance(p,str) and os.path.exists(p))).sum()\n",
    "\n",
    "print(\"Antes de filtrar por filas completas:\")\n",
    "for split, d in [(\"train\", df_tr), (\"val\", df_va), (\"test\", df_te)]:\n",
    "    for col in [\"path_g\",\"path_r\",\"path_z\"]:\n",
    "        print(f\"{split}.{col} faltantes:\", count_missing(d, col))\n",
    "\n",
    "# 3) Filtrar a filas con g/r/z presentes\n",
    "def keep_only_complete(df, name):\n",
    "    m = df.apply(lambda r: all(isinstance(r[c], str) and os.path.exists(r[c]) for c in [\"path_g\",\"path_r\",\"path_z\"]), axis=1)\n",
    "    kept = int(m.sum())\n",
    "    print(f\"[{name}] Conservando {kept}/{len(df)} filas con g/r/z presentes.\")\n",
    "    out = df[m].reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "df_tr = keep_only_complete(df_tr, \"train\")\n",
    "df_va = keep_only_complete(df_va, \"val\")\n",
    "df_te = keep_only_complete(df_te, \"test\")\n",
    "\n",
    "# 4) Deja solo columnas necesarias (más columnas de diagnóstico opcionales)\n",
    "needed = [\"image_id\",\"path_g\",\"path_r\",\"path_z\",\"label_bin\"]\n",
    "diag   = [\"_chosen_root\",\"_chosen_ext\"]  # útil para saber desde dónde salió cada fila\n",
    "df_tr = df_tr[needed + diag].copy()\n",
    "df_va = df_va[needed + diag].copy()\n",
    "df_te = df_te[needed + diag].copy()\n",
    "\n",
    "print(\"\\nResumen por split (después de filtrar):\")\n",
    "for name, d in [(\"train\", df_tr), (\"val\", df_va), (\"test\", df_te)]:\n",
    "    print(name, \"filas:\", len(d), \"| raíces usadas:\", d[\"_chosen_root\"].nunique(), \"| exts:\", d[\"_chosen_ext\"].value_counts().to_dict())\n",
    "\n",
    "# Si lo prefieres, quita las columnas de diagnóstico para seguir el pipeline:\n",
    "df_tr = df_tr[needed]\n",
    "df_va = df_va[needed]\n",
    "df_te = df_te[needed]\n",
    "\n",
    "print(\"\\nListo ✅ columnas:\", df_tr.columns.tolist(), \"| train filas:\", len(df_tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica que las rutas construidas existan\n",
    "for split, d in [(\"train\", df_tr), (\"val\", df_va), (\"test\", df_te)]:\n",
    "    for col in [\"path_g\", \"path_r\", \"path_z\"]:\n",
    "        miss = (~d[col].apply(lambda p: isinstance(p,str) and os.path.exists(p))).sum()\n",
    "        print(f\"{split}.{col} faltantes: {miss}\")\n",
    "\n",
    "# Muestra algunos ejemplos de rutas\n",
    "print(\"\\nEjemplo de rutas generadas:\")\n",
    "print(df_tr[[\"image_id\", \"path_g\", \"path_r\", \"path_z\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOqXHzXSEdg5"
   },
   "source": [
    "##Pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "\n",
    "def read_fits_first2d(path: str) -> np.ndarray:\n",
    "    arr = None\n",
    "    with fits.open(path, memmap=False) as hdul:\n",
    "        for h in hdul:\n",
    "            if getattr(h, \"data\", None) is not None and getattr(h.data, \"ndim\", 0) == 2:\n",
    "                arr = h.data.astype(np.float32)\n",
    "                break\n",
    "    if arr is None:\n",
    "        raise ValueError(f\"Sin HDU 2D en {path}\")\n",
    "    return np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def read_image_any(path: str) -> np.ndarray:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".fits\", \".fit\", \".fz\"]:\n",
    "        return read_fits_first2d(path)\n",
    "    img = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"No se pudo leer {path}\")\n",
    "    if img.ndim == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "def center_pad_square(img: np.ndarray, target: int = 256) -> np.ndarray:\n",
    "    h, w = img.shape[:2]\n",
    "    s = max(h, w)\n",
    "    pad_top = (s - h) // 2\n",
    "    pad_left = (s - w) // 2\n",
    "    canvas = np.zeros((s, s), dtype=img.dtype)\n",
    "    canvas[pad_top:pad_top+h, pad_left:pad_left+w] = img\n",
    "    return cv2.resize(canvas, (target, target), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def asinh_stretch(x: np.ndarray, nonlinearity: float = 0.1) -> np.ndarray:\n",
    "    x = x - np.median(x)\n",
    "    x = x / (np.std(x) + 1e-6)\n",
    "    x = np.arcsinh(nonlinearity * x)\n",
    "    x = x - x.min()\n",
    "    return x / (x.max() + 1e-8)\n",
    "\n",
    "def preprocess_image(path: str, size: int = 256) -> np.ndarray:\n",
    "    img = read_image_any(path)\n",
    "    img = center_pad_square(img, target=size)\n",
    "    img = asinh_stretch(img, nonlinearity=0.1)\n",
    "    return img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwwaZDTyEl-w"
   },
   "source": [
    "##Entrenamiento / feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "\n",
    "def read_fits_first2d(path: str) -> np.ndarray:\n",
    "    arr = None\n",
    "    with fits.open(path, memmap=False) as hdul:\n",
    "        for h in hdul:\n",
    "            if getattr(h, \"data\", None) is not None and getattr(h.data, \"ndim\", 0) == 2:\n",
    "                arr = h.data.astype(np.float32)\n",
    "                break\n",
    "    if arr is None:\n",
    "        raise ValueError(f\"Sin HDU 2D en {path}\")\n",
    "    return np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def read_image_any(path: str) -> np.ndarray:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".fits\", \".fit\", \".fz\"]:\n",
    "        return read_fits_first2d(path)\n",
    "    img = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"No se pudo leer {path}\")\n",
    "    if img.ndim == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "def center_pad_square(img: np.ndarray, target: int = 256) -> np.ndarray:\n",
    "    h, w = img.shape[:2]\n",
    "    s = max(h, w)\n",
    "    pad_top = (s - h) // 2\n",
    "    pad_left = (s - w) // 2\n",
    "    canvas = np.zeros((s, s), dtype=img.dtype)\n",
    "    canvas[pad_top:pad_top+h, pad_left:pad_left+w] = img\n",
    "    return cv2.resize(canvas, (target, target), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def asinh_stretch(x: np.ndarray, nonlinearity: float = 0.1) -> np.ndarray:\n",
    "    x = x - np.median(x)\n",
    "    x = x / (np.std(x) + 1e-6)\n",
    "    x = np.arcsinh(nonlinearity * x)\n",
    "    x = x - x.min()\n",
    "    return x / (x.max() + 1e-8)\n",
    "\n",
    "def preprocess_image(path: str, size: int = 256) -> np.ndarray:\n",
    "    img = read_image_any(path)\n",
    "    img = center_pad_square(img, target=size)\n",
    "    img = asinh_stretch(img, nonlinearity=0.1)\n",
    "    return img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fIsOf4rEsqp"
   },
   "source": [
    "##Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "linear_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"saga\",\n",
    "        max_iter=3000,\n",
    "        C=1.0,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "def dist(name, fe):\n",
    "    print(name, \"shape:\", fe.shape)\n",
    "    if \"label_bin\" in fe.columns:\n",
    "        print(fe[\"label_bin\"].value_counts(dropna=False).to_dict())\n",
    "    else:\n",
    "        print(\"No 'label_bin' in features!\")\n",
    "\n",
    "dist(\"fe_tr\", fe_tr)\n",
    "dist(\"fe_va\", fe_va)\n",
    "dist(\"fe_te\", fe_te)\n",
    "\n",
    "\n",
    "linear_clf.fit(X_tr, y_tr)\n",
    "\n",
    "# Umbral óptimo por F1 en validación\n",
    "val_proba = linear_clf.predict_proba(X_va)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_va, val_proba)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = int(np.nanargmax(f1s))\n",
    "best_thr = thr[max(0, best_idx-1)] if best_idx >= 1 else 0.5\n",
    "print(f\"Umbral óptimo (val) por F1: {best_thr:.4f} | F1={f1s[best_idx]:.4f}\")\n",
    "\n",
    "# Evaluación en test\n",
    "test_proba = linear_clf.predict_proba(X_te)[:,1]\n",
    "test_pred  = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\":  float(accuracy_score(y_te, test_pred)),\n",
    "    \"precision\": float(precision_score(y_te, test_pred, zero_division=0)),\n",
    "    \"recall\":    float(recall_score(y_te, test_pred, zero_division=0)),\n",
    "    \"f1\":        float(f1_score(y_te, test_pred, zero_division=0)),\n",
    "    \"auroc\":     float(roc_auc_score(y_te, test_proba)),\n",
    "    \"auprc\":     float(average_precision_score(y_te, test_proba)),\n",
    "    \"best_thr\":  float(best_thr)\n",
    "}\n",
    "print(\"Métricas en test:\", metrics)\n",
    "\n",
    "# Curvas y matrices\n",
    "fpr, tpr, _ = roc_curve(y_te, test_proba)\n",
    "pr_p, pr_r, _ = precision_recall_curve(y_te, test_proba)\n",
    "cm = confusion_matrix(y_te, test_pred)\n",
    "cm_norm = confusion_matrix(y_te, test_pred, normalize=\"true\")\n",
    "\n",
    "# Importancias lineales (coeficientes)\n",
    "coefs = pd.Series(linear_clf.named_steps[\"logreg\"].coef_.ravel(), index=feature_cols)         .sort_values(key=np.abs, ascending=False)\n",
    "print(\"Top-10 |coef|:\", coefs.head(10).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULIOffAwMu1k"
   },
   "source": [
    "Matriz de Confusion:\n",
    "                Pred: Sin barra | Con barra\n",
    "Real: Sin barra        715          96\n",
    "      Con barra        228         477\n",
    "\n",
    "Metricas :\n",
    "  Accuracy:  0.6234\n",
    "  Precision: 0.6124\n",
    "  Recall:    0.6510\n",
    "  F1-Score:  0.6309\n",
    "  AUPRC:     0.6878\n",
    "  AUROC:     0.7006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdTV_qCCEzua"
   },
   "source": [
    "##Output y guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = os.path.join(directory, \"linear_results\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Modelo/pipeline con metadatos\n",
    "with open(os.path.join(OUT_DIR, \"linear_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"pipeline\": linear_clf,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"best_threshold\": best_thr,\n",
    "        \"seed\": SEED\n",
    "    }, f)\n",
    "\n",
    "# Coefs\n",
    "coefs.to_csv(os.path.join(OUT_DIR, \"coef_importance.csv\"), header=[\"coef\"], index=True)\n",
    "\n",
    "# Curvas\n",
    "val_fpr, val_tpr, _ = roc_curve(y_va, val_proba)\n",
    "val_pr_p, val_pr_r, _ = precision_recall_curve(y_va, val_proba)\n",
    "\n",
    "pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr}).to_csv(os.path.join(OUT_DIR, \"test_roc_curve.csv\"), index=False)\n",
    "pd.DataFrame({\"precision\": pr_p, \"recall\": pr_r}).to_csv(os.path.join(OUT_DIR, \"test_pr_curve.csv\"), index=False)\n",
    "pd.DataFrame({\"fpr\": val_fpr, \"tpr\": val_tpr}).to_csv(os.path.join(OUT_DIR, \"val_roc_curve.csv\"), index=False)\n",
    "pd.DataFrame({\"precision\": val_pr_p, \"recall\": val_pr_r}).to_csv(os.path.join(OUT_DIR, \"val_pr_curve.csv\"), index=False)\n",
    "\n",
    "# Resultados por imagen (test)\n",
    "pd.DataFrame({\n",
    "    \"image_id\": fe_te[\"image_id\"].values,\n",
    "    \"label\": y_te,\n",
    "    \"proba_bar\": test_proba,\n",
    "    \"pred_bar\": test_pred\n",
    "}).to_csv(os.path.join(OUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "# Matrices de confusión\n",
    "pd.DataFrame(cm, index=[\"NoBar\",\"Bar\"], columns=[\"Pred_NoBar\",\"Pred_Bar\"])\\\n",
    "  .to_csv(os.path.join(OUT_DIR, \"confusion_matrix.csv\"))\n",
    "pd.DataFrame(cm_norm, index=[\"NoBar\",\"Bar\"], columns=[\"Pred_NoBar\",\"Pred_Bar\"])\\\n",
    "  .to_csv(os.path.join(OUT_DIR, \"confusion_matrix_norm.csv\"))\n",
    "\n",
    "# Métricas resumen\n",
    "with open(os.path.join(OUT_DIR, \"metrics_test.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Artefactos guardados en:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e51sY4FMG4EI"
   },
   "source": [
    "# Modelo 6: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYMHWsfjE8lw"
   },
   "source": [
    "##Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, json, gc, math, pickle, warnings, random\n",
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 2) Rutas y parámetros\n",
    "directory = \"/content/drive/MyDrive/Proyecto_Integrador/Deteccion/LN/Dataset\"\n",
    "IMAGE_ROOTS = [\n",
    "    \"/content/drive/MyDrive/Proyecto_Integrador/data/images\",\n",
    "    \"/content/drive/MyDrive/Proyecto_Integrador/Deteccion/LN/Images\",\n",
    "]\n",
    "print(\"Fecha:\", datetime.now()); print(\"Splits:\", directory); print(\"Roots:\\n - \" + \"\\n - \".join(IMAGE_ROOTS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_raw(base_dir: str, stem: str) -> pd.DataFrame:\n",
    "    p_parquet = os.path.join(base_dir, f\"{stem}.parquet\")\n",
    "    p_csv     = os.path.join(base_dir, f\"{stem}.csv\")\n",
    "    p = p_parquet if os.path.exists(p_parquet) else (p_csv if os.path.exists(p_csv) else None)\n",
    "    if p is None: raise FileNotFoundError(f\"Falta {stem}.parquet/.csv en {base_dir}\")\n",
    "    return pd.read_parquet(p) if p.endswith(\".parquet\") else pd.read_csv(p)\n",
    "\n",
    "def normalize_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"label_bin\" not in df.columns:\n",
    "        if \"label\" in df.columns:\n",
    "            df[\"label_bin\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).round().astype(int)\n",
    "        elif \"label_str\" in df.columns:\n",
    "            df[\"label_bin\"] = pd.to_numeric(df[\"label_str\"], errors=\"coerce\").fillna(0).round().astype(int)\n",
    "        elif \"Bars\" in df.columns:\n",
    "            v = df[\"Bars\"].astype(str).str.lower()\n",
    "            df[\"label_bin\"] = v.isin([\"1\",\"true\",\"yes\",\"si\",\"sí\",\"bar\",\"bars\"]).astype(int)\n",
    "        else:\n",
    "            raise ValueError(\"No hay label_bin/label/label_str/Bars\")\n",
    "    return df\n",
    "\n",
    "df_tr_raw = normalize_labels(load_split_raw(directory, \"train\"))\n",
    "df_va_raw = normalize_labels(load_split_raw(directory, \"val\"))\n",
    "df_te_raw = normalize_labels(load_split_raw(directory, \"test\"))\n",
    "\n",
    "print(\"Distribución RAW:\")\n",
    "for name, df in [(\"train\", df_tr_raw), (\"val\", df_va_raw), (\"test\", df_te_raw)]:\n",
    "    print(name, df[\"label_bin\"].value_counts(dropna=False).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "td08JfZtE_IW"
   },
   "source": [
    "##Pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxs = [\n",
    "    (\"suffix_us\",   re.compile(r\"^(?P<tok>.+)_(?P<band>[grz])\\.(?P<ext>fits|fz|fit|png|jpg|jpeg)$\", re.I)),\n",
    "    (\"suffix_dash\", re.compile(r\"^(?P<tok>.+)-(?P<band>[grz])\\.(?P<ext>fits|fz|fit|png|jpg|jpeg)$\", re.I)),\n",
    "    (\"suffix_dot\",  re.compile(r\"^(?P<tok>.+)\\.(?P<band>[grz])\\.(?P<ext>fits|fz|fit|png|jpg|jpeg)$\", re.I)),\n",
    "    (\"prefix_us\",   re.compile(r\"^(?P<band>[grz])_(?P<tok>.+)\\.(?P<ext>fits|fz|fit|png|jpg|jpeg)$\", re.I)),\n",
    "]\n",
    "\n",
    "def split_name(fullpath):\n",
    "    fn = os.path.basename(fullpath)\n",
    "    base, ext = os.path.splitext(fn); ext = ext[1:].lower()\n",
    "    for tag, rx in rxs:\n",
    "        m = rx.match(fn.lower())\n",
    "        if m: return m.group(\"tok\"), m.group(\"band\").lower(), m.group(\"ext\").lower(), tag\n",
    "    parts = os.path.normpath(fullpath).replace(\"\\\\\",\"/\").split(\"/\")\n",
    "    if len(parts) >= 2 and parts[-2].lower() in {\"g\",\"r\",\"z\"}:\n",
    "        return base, parts[-2].lower(), ext, \"subfolder\"\n",
    "    return None, None, None, None\n",
    "\n",
    "idx = {}; stats = {}; total_files = 0\n",
    "for ROOT in IMAGE_ROOTS:\n",
    "    if not os.path.exists(ROOT): continue\n",
    "    for root, _, files in os.walk(ROOT):\n",
    "        for fn in files:\n",
    "            low = fn.lower()\n",
    "            if not low.endswith((\".fits\",\".fz\",\".fit\",\".png\",\".jpg\",\".jpeg\")): continue\n",
    "            full = os.path.join(root, fn)\n",
    "            tok, band, ext, tag = split_name(full)\n",
    "            if not tok or not band: continue\n",
    "            key = (tok, band)\n",
    "            if key not in idx or ext in {\"fits\",\"fz\",\"fit\"}:\n",
    "                idx[key] = full\n",
    "            stats[(tag or \"unknown\", ext or \"unknown\")] = stats.get((tag or \"unknown\", ext or \"unknown\"), 0) + 1\n",
    "            total_files += 1\n",
    "\n",
    "print(\"Indexados:\", total_files)\n",
    "for (tag, ext), n in sorted(stats.items(), key=lambda x: -x[1])[:8]:\n",
    "    print(f\"{tag} .{ext} -> {n}\")\n",
    "\n",
    "def resolve_variants(tok: str):\n",
    "    return [tok, tok.replace(\" \", \"_\"), tok.replace(\"_\",\"-\"), tok.replace(\"-\",\"_\")]\n",
    "\n",
    "def build_paths_from_index(df_raw: pd.DataFrame):\n",
    "    df = df_raw.copy()\n",
    "    df[\"image_id\"] = df.get(\"image_id\", df.get(\"name\")).astype(str)\n",
    "    G, R, Z = [], [], []\n",
    "    for tok in df[\"image_id\"].tolist():\n",
    "        g = r = z = None\n",
    "        for t in resolve_variants(tok):\n",
    "            g = idx.get((t,\"g\"), g)\n",
    "            r = idx.get((t,\"r\"), r)\n",
    "            z = idx.get((t,\"z\"), z)\n",
    "            if g and r and z: break\n",
    "        G.append(g); R.append(r); Z.append(z)\n",
    "    df[\"path_g\"] = G; df[\"path_r\"] = R; df[\"path_z\"] = Z\n",
    "    return df\n",
    "\n",
    "df_tr = build_paths_from_index(df_tr_raw)\n",
    "df_va = build_paths_from_index(df_va_raw)\n",
    "df_te = build_paths_from_index(df_te_raw)\n",
    "\n",
    "def count_missing(df, col):\n",
    "    return (~df[col].apply(lambda p: isinstance(p,str) and os.path.exists(p))).sum()\n",
    "\n",
    "print(\"Faltantes antes de filtrar:\")\n",
    "for name, d in [(\"train\", df_tr), (\"val\", df_va), (\"test\", df_te)]:\n",
    "    print(name, count_missing(d,\"path_g\"), count_missing(d,\"path_r\"), count_missing(d,\"path_z\"))\n",
    "\n",
    "def keep_only_complete(df, name):\n",
    "    m = df.apply(lambda r: all(isinstance(r[c], str) and os.path.exists(r[c]) for c in [\"path_g\",\"path_r\",\"path_z\"]), axis=1)\n",
    "    kept = int(m.sum()); print(f\"{name} completos: {kept}/{len(df)}\")\n",
    "    return df[m].reset_index(drop=True)\n",
    "\n",
    "df_tr = keep_only_complete(df_tr, \"train\")\n",
    "df_va = keep_only_complete(df_va, \"val\")\n",
    "df_te = keep_only_complete(df_te, \"test\")\n",
    "\n",
    "needed = [\"image_id\",\"path_g\",\"path_r\",\"path_z\",\"label_bin\"]\n",
    "df_tr = df_tr[needed]; df_va = df_va[needed]; df_te = df_te[needed]\n",
    "\n",
    "print(\"Distribución tras rutas:\")\n",
    "for name, d in [(\"train\", df_tr), (\"val\", df_va), (\"test\", df_te)]:\n",
    "    print(name, d[\"label_bin\"].value_counts(dropna=False).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from astropy.io import fits\n",
    "\n",
    "def read_fits_first2d(path: str) -> np.ndarray:\n",
    "    arr = None\n",
    "    with fits.open(path, memmap=False) as hdul:\n",
    "        for h in hdul:\n",
    "            if getattr(h, \"data\", None) is not None and getattr(h.data, \"ndim\", 0) == 2:\n",
    "                arr = h.data.astype(np.float32); break\n",
    "    if arr is None: raise ValueError(f\"Sin HDU 2D: {path}\")\n",
    "    return np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def read_image_any(path: str) -> np.ndarray:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".fits\",\".fit\",\".fz\"]:\n",
    "        return read_fits_first2d(path)\n",
    "    img = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    if img is None: img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None: raise ValueError(f\"No se pudo leer {path}\")\n",
    "    if img.ndim == 3: img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "def center_pad_square(img: np.ndarray, target: int = 256) -> np.ndarray:\n",
    "    h, w = img.shape[:2]; s = max(h, w)\n",
    "    pad_top = (s - h) // 2; pad_left = (s - w) // 2\n",
    "    canvas = np.zeros((s, s), dtype=img.dtype)\n",
    "    canvas[pad_top:pad_top+h, pad_left:pad_left+w] = img\n",
    "    return cv2.resize(canvas, (target, target), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def asinh_stretch(x: np.ndarray, nonlinearity: float = 0.1) -> np.ndarray:\n",
    "    x = x - np.median(x); x = x / (np.std(x) + 1e-6); x = np.arcsinh(nonlinearity * x)\n",
    "    x = x - x.min(); return x / (x.max() + 1e-8)\n",
    "\n",
    "def preprocess_image(path: str, size: int = 256) -> np.ndarray:\n",
    "    img = read_image_any(path); img = center_pad_square(img, target=size); img = asinh_stretch(img, 0.1)\n",
    "    return img.astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaTo2iTjFLux"
   },
   "source": [
    "##Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_stats(x: np.ndarray) -> Dict[str, float]:\n",
    "    flat = x.reshape(-1)\n",
    "    q01, q05, q25, q50, q75, q95, q99 = np.percentile(flat, [1,5,25,50,75,95,99])\n",
    "    mu, sigma = float(flat.mean()), float(flat.std())\n",
    "    return {\n",
    "        \"min\": float(flat.min()), \"p01\": float(q01), \"p05\": float(q05),\n",
    "        \"p25\": float(q25), \"p50\": float(q50), \"p75\": float(q75),\n",
    "        \"p95\": float(q95), \"p99\": float(q99), \"max\": float(flat.max()),\n",
    "        \"mean\": mu, \"std\": sigma, \"skew\": float(((flat - mu)**3).mean() / (sigma**3 + 1e-8)),\n",
    "        \"kurt\": float(((flat - mu)**4).mean() / (sigma**4 + 1e-8)),\n",
    "    }\n",
    "\n",
    "def entropy_gray(x: np.ndarray, bins: int = 64) -> float:\n",
    "    hist, _ = np.histogram(x, bins=bins, range=(0.0, 1.0), density=True)\n",
    "    p = hist / (hist.sum() + 1e-12)\n",
    "    return float(-(p[p>0] * np.log(p[p>0] + 1e-12)).sum())\n",
    "\n",
    "def grad_stats(x: np.ndarray) -> Dict[str, float]:\n",
    "    gx = cv2.Sobel(x, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(x, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    mag = np.hypot(gx, gy); ang = (np.arctan2(gy, gx) + np.pi) % (2*np.pi)\n",
    "    s_mag = robust_stats(mag)\n",
    "    out = {f\"grad_{k}\": v for k, v in s_mag.items()}\n",
    "    out[\"grad_entropy\"] = entropy_gray(mag); out[\"ang_mean\"] = float(ang.mean()); out[\"ang_std\"] = float(ang.std())\n",
    "    return out\n",
    "\n",
    "def safe_corr(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    c = np.corrcoef(a.reshape(-1), b.reshape(-1))[0,1]; return 0.0 if np.isnan(c) else float(c)\n",
    "\n",
    "def band_features(img: np.ndarray, band: str) -> Dict[str, float]:\n",
    "    st = robust_stats(img); gr = grad_stats(img); ent = entropy_gray(img)\n",
    "    feats = {f\"{band}_{k}\": v for k, v in st.items()}\n",
    "    feats.update({f\"{band}_{k}\": v for k, v in gr.items()})\n",
    "    feats[f\"{band}_entropy\"] = ent\n",
    "    return feats\n",
    "\n",
    "def color_features(g: np.ndarray, r: np.ndarray, z: np.ndarray) -> Dict[str, float]:\n",
    "    gmag = -2.5 * np.log10(g + 1e-6); rmag = -2.5 * np.log10(r + 1e-6); zmag = -2.5 * np.log10(z + 1e-6)\n",
    "    gr = gmag - rmag; rz = rmag - zmag\n",
    "    feats = {}\n",
    "    for arr, name in [(gr, \"gmr\"), (rz, \"rmz\")]:\n",
    "        st = robust_stats(arr); feats.update({f\"color_{name}_{k}\": v for k, v in st.items()})\n",
    "        feats[f\"color_{name}_entropy\"] = entropy_gray((arr - arr.min())/(arr.max()-arr.min()+1e-8))\n",
    "    feats[\"corr_gr\"] = safe_corr(g, r); feats[\"corr_rz\"] = safe_corr(r, z); feats[\"corr_gz\"] = safe_corr(g, z)\n",
    "    return feats\n",
    "\n",
    "def extract_features_row(row: pd.Series, size: int = 256) -> Dict[str, float]:\n",
    "    g = preprocess_image(row[\"path_g\"], size); r = preprocess_image(row[\"path_r\"], size); z = preprocess_image(row[\"path_z\"], size)\n",
    "    feats = {}; feats.update(band_features(g, \"g\")); feats.update(band_features(r, \"r\")); feats.update(band_features(z, \"z\")); feats.update(color_features(g, r, z))\n",
    "    feats[\"image_id\"] = row[\"image_id\"]; feats[\"label_bin\"] = int(row[\"label_bin\"]); return feats\n",
    "\n",
    "def extract_features(df: pd.DataFrame, size: int = 256, desc: str = \"\") -> pd.DataFrame:\n",
    "    recs = []\n",
    "    for i, row in df.iterrows():\n",
    "        try: recs.append(extract_features_row(row, size=size))\n",
    "        except Exception as e: print(f\"[{desc}] fila {i} {row.get('image_id','?')}: {e}\")\n",
    "    out = pd.DataFrame(recs); return out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "print(\"Extrayendo features...\")\n",
    "fe_tr = extract_features(df_tr, size=256, desc=\"train\")\n",
    "fe_va = extract_features(df_va, size=256, desc=\"val\")\n",
    "fe_te = extract_features(df_te, size=256, desc=\"test\")\n",
    "\n",
    "if \"label_bin\" not in fe_tr.columns: fe_tr = fe_tr.merge(df_tr[[\"image_id\",\"label_bin\"]], on=\"image_id\", how=\"left\")\n",
    "if \"label_bin\" not in fe_va.columns: fe_va = fe_va.merge(df_va[[\"image_id\",\"label_bin\"]], on=\"image_id\", how=\"left\")\n",
    "if \"label_bin\" not in fe_te.columns: fe_te = fe_te.merge(df_te[[\"image_id\",\"label_bin\"]], on=\"image_id\", how=\"left\")\n",
    "\n",
    "if fe_tr.empty or fe_va.empty or fe_te.empty: raise RuntimeError(\"Features vacíos: revisa rutas e IO.\")\n",
    "\n",
    "print(\"Shapes features:\", fe_tr.shape, fe_va.shape, fe_te.shape)\n",
    "print(\"Distribución en features:\")\n",
    "for name, fe in [(\"train\", fe_tr), (\"val\", fe_va), (\"test\", fe_te)]:\n",
    "    print(name, fe[\"label_bin\"].value_counts(dropna=False).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20in3xksFPhe"
   },
   "source": [
    "##Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_col = \"label_bin\"\n",
    "meta_cols  = [\"image_id\", target_col]\n",
    "feature_cols = [c for c in fe_tr.columns if c not in meta_cols]\n",
    "\n",
    "fe_va = fe_va.reindex(columns=feature_cols + meta_cols, fill_value=0.0)\n",
    "fe_te = fe_te.reindex(columns=feature_cols + meta_cols, fill_value=0.0)\n",
    "\n",
    "X_tr, y_tr = fe_tr[feature_cols].values, fe_tr[target_col].values\n",
    "X_va, y_va = fe_va[feature_cols].values, fe_va[target_col].values\n",
    "X_te, y_te = fe_te[feature_cols].values, fe_te[target_col].values\n",
    "\n",
    "if len(np.unique(y_tr)) < 2:\n",
    "    fe_all = pd.concat([fe_tr.assign(_s=\"train\"), fe_va.assign(_s=\"val\"), fe_te.assign(_s=\"test\")], ignore_index=True)\n",
    "    if fe_all[target_col].nunique() < 2: raise RuntimeError(\"Features con una sola clase.\")\n",
    "    test_val_ratio = (len(fe_va)+len(fe_te))/len(fe_all)\n",
    "    fe_train, fe_rest = train_test_split(fe_all, test_size=test_val_ratio, stratify=fe_all[target_col], random_state=SEED)\n",
    "    te_ratio = len(fe_te)/max(1,(len(fe_va)+len(fe_te)))\n",
    "    fe_val, fe_test = train_test_split(fe_rest, test_size=te_ratio, stratify=fe_rest[target_col], random_state=SEED)\n",
    "    feature_cols = [c for c in fe_train.columns if c not in [\"image_id\", target_col, \"_s\"]]\n",
    "    X_tr, y_tr = fe_train[feature_cols].values, fe_train[target_col].values\n",
    "    X_va, y_va = fe_val[feature_cols].values,   fe_val[target_col].values\n",
    "    X_te, y_te = fe_test[feature_cols].values,  fe_test[target_col].values\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_tr, y_tr)\n",
    "val_proba = rf.predict_proba(X_va)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_va, val_proba)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = int(np.nanargmax(f1s))\n",
    "best_thr = thr[max(0, best_idx-1)] if best_idx >= 1 else 0.5\n",
    "print(\"Umbral F1(val):\", round(best_thr,4), \"F1*:\", round(float(f1s[best_idx]),4))\n",
    "\n",
    "test_proba = rf.predict_proba(X_te)[:,1]\n",
    "test_pred  = (test_proba >= best_thr).astype(int)\n",
    "metrics = {\n",
    "    \"accuracy\":  float(accuracy_score(y_te, test_pred)),\n",
    "    \"precision\": float(precision_score(y_te, test_pred, zero_division=0)),\n",
    "    \"recall\":    float(recall_score(y_te, test_pred, zero_division=0)),\n",
    "    \"f1\":        float(f1_score(y_te, test_pred, zero_division=0)),\n",
    "    \"auroc\":     float(roc_auc_score(y_te, test_proba)),\n",
    "    \"auprc\":     float(average_precision_score(y_te, test_proba)),\n",
    "    \"best_thr\":  float(best_thr)\n",
    "}\n",
    "print(\"Métricas test:\", metrics)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_te, test_proba)\n",
    "pr_p, pr_r, _ = precision_recall_curve(y_te, test_proba)\n",
    "cm = confusion_matrix(y_te, test_pred)\n",
    "cm_norm = confusion_matrix(y_te, test_pred, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmug3BJmNSdZ"
   },
   "source": [
    "Matriz de Confusion:\n",
    "                Pred: Sin barra | Con barra\n",
    "Real: Sin barra        737          81\n",
    "      Con barra        213         455\n",
    "\n",
    "Metricas :\n",
    "  Accuracy:  0.6522\n",
    "  Precision: 0.6493\n",
    "  Recall:    0.6753\n",
    "  F1-Score:  0.6490\n",
    "  AUPRC:     0.7089\n",
    "  AUROC:     0.7287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NzMo8xaFRl1"
   },
   "source": [
    "##Output y Guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = os.path.join(directory, \"rf_results_onecell\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"rf_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"model\": rf, \"feature_cols\": feature_cols, \"best_threshold\": best_thr, \"seed\": SEED}, f)\n",
    "\n",
    "imp = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "imp.to_csv(os.path.join(OUT_DIR, \"feature_importances.csv\"))\n",
    "\n",
    "pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr}).to_csv(os.path.join(OUT_DIR, \"test_roc_curve.csv\"), index=False)\n",
    "pd.DataFrame({\"precision\": pr_p, \"recall\": pr_r}).to_csv(os.path.join(OUT_DIR, \"test_pr_curve.csv\"), index=False)\n",
    "val_fpr, val_tpr, _ = roc_curve(y_va, val_proba); val_pr_p, val_pr_r, _ = precision_recall_curve(y_va, val_proba)\n",
    "pd.DataFrame({\"fpr\": val_fpr, \"tpr\": val_tpr}).to_csv(os.path.join(OUT_DIR, \"val_roc_curve.csv\"), index=False)\n",
    "pd.DataFrame({\"precision\": val_pr_p, \"recall\": val_pr_r}).to_csv(os.path.join(OUT_DIR, \"val_pr_curve.csv\"), index=False)\n",
    "pd.DataFrame({\"image_id\": fe_te[\"image_id\"].values, \"label\": y_te, \"proba_bar\": test_proba, \"pred_bar\": test_pred}).to_csv(os.path.join(OUT_DIR, \"test_results.csv\"), index=False)\n",
    "pd.DataFrame(cm, index=[\"NoBar\",\"Bar\"], columns=[\"Pred_NoBar\",\"Pred_Bar\"]).to_csv(os.path.join(OUT_DIR, \"confusion_matrix.csv\"))\n",
    "pd.DataFrame(cm_norm, index=[\"NoBar\",\"Bar\"], columns=[\"Pred_NoBar\",\"Pred_Bar\"]).to_csv(os.path.join(OUT_DIR, \"confusion_matrix_norm.csv\"))\n",
    "with open(os.path.join(OUT_DIR, \"metrics_test.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Artefactos en:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDJroI_GHIN7"
   },
   "source": [
    "# Comparativa de Resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGpy8z_bNsLg"
   },
   "source": [
    "**1.1 Tabla Resumen de Métricas**\n",
    "\n",
    "| Modelo | Tipo | Accuracy | Precision | Recall | F1-Score | AUROC | AUPRC |\n",
    "|--------|------|----------|-----------|--------|----------|-------|-------|\n",
    "| **EfficientNet-B0** | Deep Learning (CNN) | **0.8466** | **0.7918** | 0.7271 | **0.7580** | **0.9210** | **0.8693** |\n",
    "| **Random Forest** | Ensemble | 0.6522 | 0.6493 | 0.6753 | 0.6490 | 0.7287 | 0.7089 |\n",
    "| **Regresión Lineal** | Lineal | 0.6234 | 0.6124 | 0.6510 | 0.6309 | 0.7006 | 0.6878 |\n",
    "| **CatBoost** | Gradient Boosting | 0.6897 | 0.5503 | 0.3273 | 0.4105 | 0.7291 | 0.5396 |\n",
    "| **XGBoost** | Gradient Boosting | 0.6076 | 0.4472 | **0.7928** | 0.5718 | 0.7264 | 0.5434 |\n",
    "| SVM (HOG+PCA)* | Clasificador Clásico | 0.72 | 0.65 | 0.58 | 0.61 | 0.78 | 0.68 |\n",
    "\n",
    "\n",
    "\n",
    "**1.2 Análisis de Matrices de Confusión**\n",
    "\n",
    "**EfficientNet-B0:**\n",
    "```\n",
    "                    Predicción\n",
    "                Sin Barra    Con Barra\n",
    "Real  Sin Barra     921         96\n",
    "      Con Barra     137        365\n",
    "\n",
    "- Falsos Positivos: 96 (9.4% de negativos)\n",
    "- Falsos Negativos: 137 (27.3% de positivos)\n",
    "- Balance superior en ambas clases\n",
    "```\n",
    "\n",
    "**XGBoost:**\n",
    "```\n",
    "                    Predicción\n",
    "                Sin Barra    Con Barra\n",
    "Real  Sin Barra     525        492\n",
    "      Con Barra     104        398\n",
    "\n",
    "- Falsos Positivos: 492 (48.4% de negativos)\n",
    "- Falsos Negativos: 104 (20.7% de positivos)\n",
    "- Alta sensibilidad, baja especificidad\n",
    "```\n",
    "\n",
    "**Random Forest:**\n",
    "```\n",
    "                    Predicción\n",
    "                Sin Barra    Con Barra\n",
    "Real  Sin Barra     737         81\n",
    "      Con Barra     213        455\n",
    "\n",
    "- Falsos Positivos: 81 (9.9% de negativos)\n",
    "- Falsos Negativos: 213 (31.9% de positivos)\n",
    "- Balance intermedio, mejor especificidad que XGBoost\n",
    "```\n",
    "\n",
    "**Regresión Lineal:**\n",
    "```\n",
    "                    Predicción\n",
    "                Sin Barra    Con Barra\n",
    "Real  Sin Barra     715         96\n",
    "      Con Barra     228        477\n",
    "\n",
    "- Falsos Positivos: 96 (11.8% de negativos)\n",
    "- Falsos Negativos: 228 (32.3% de positivos)\n",
    "- Performance limitada por simplicidad del modelo\n",
    "```\n",
    "\n",
    "**1.3 Fortalezas y Debilidades por Modelo**\n",
    "\n",
    "EfficientNet-B0 (CNN)\n",
    "\n",
    "**Fortalezas:**\n",
    "- Mejor balance precision-recall (F1=0.758)\n",
    "- Discriminación de clases sobresaliente (AUROC=0.921)\n",
    "- Captura patrones morfológicos complejos automáticamente\n",
    "- Excelente generalización (AUPRC=0.869)\n",
    "- Robusto ante variabilidad en imágenes astronómicas\n",
    "- No requiere feature engineering manual\n",
    "\n",
    "**Debilidades:**\n",
    "- Mayor tiempo de entrenamiento (~30 minutos)\n",
    "- Requiere GPU para inferencia eficiente\n",
    "- 5.3M parámetros (mayor huella de memoria)\n",
    "- Menor interpretabilidad (caja negra)\n",
    "- Necesita más datos para entrenamiento óptimo\n",
    "\n",
    "XGBoost (Gradient Boosting)\n",
    "\n",
    "**Fortalezas:**\n",
    "- Recall excepcional (0.793) - detecta más barras\n",
    "- Entrenamiento rápido (~2 minutos)\n",
    "- Altamente interpretable (importancia de features)\n",
    "- Eficiente computacionalmente (CPU)\n",
    "- Menor requerimiento de datos\n",
    "- Fácil deployment\n",
    "\n",
    "**Debilidades:**\n",
    "- Precision baja (0.447) - muchos falsos positivos\n",
    "- F1 moderado (0.572)\n",
    "- Depende de la calidad del feature engineering\n",
    "- Menor capacidad para patrones complejos\n",
    "- Sensible a desbalance de clases\n",
    "\n",
    "CatBoost\n",
    "\n",
    "**Fortalezas:**\n",
    "- Entrenamiento más rápido (~1 minuto)\n",
    "- Manejo nativo de features categóricas\n",
    "- Menos propenso a overfitting\n",
    "\n",
    "**Debilidades:**\n",
    "- Recall muy bajo (0.327) - pierde muchas barras\n",
    "- Performance general inferior a los top 2\n",
    "- No ofrece ventajas significativas en este problema\n",
    "\n",
    "Random Forest\n",
    "\n",
    "**Fortalezas:**\n",
    "- Balance razonable precision-recall (F1=0.649)\n",
    "- Buena especificidad (solo 9.9% falsos positivos)\n",
    "- Interpretable mediante importancia de features\n",
    "- Robusto ante overfitting\n",
    "- No requiere tuning extensivo\n",
    "\n",
    "**Debilidades:**\n",
    "- Recall moderado (0.675) - pierde ~32% de barras\n",
    "- AUROC limitado (0.729) comparado con CNN\n",
    "- Requiere feature engineering manual\n",
    "- Performance general superada por CNN\n",
    "\n",
    "Regresión Lineal\n",
    "\n",
    "**Fortalezas:**\n",
    "- Extremadamente rápido (<1 minuto)\n",
    "- Máxima interpretabilidad (coeficientes lineales)\n",
    "- Bajo costo computacional\n",
    "- Útil como baseline\n",
    "\n",
    "**Debilidades:**\n",
    "- Limitado por asunciones lineales\n",
    "- F1 bajo (0.631) - inadecuado para producción\n",
    "- No captura interacciones complejas\n",
    "- Recall insuficiente (0.651) para aplicaciones científicas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30ajquncI5W6"
   },
   "source": [
    "# Selección de los 2 Mejores Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33n6af8lOT82"
   },
   "source": [
    "**2.1 Criterios de Selección**\n",
    "\n",
    "Para esta tarea astronómica, priorizamos:\n",
    "\n",
    "1. **Balance Precision-Recall (F1)** - 30%\n",
    "   - Crítico para minimizar trabajo manual de verificación\n",
    "   \n",
    "2. **Discriminación (AUROC)** - 25%\n",
    "   - Capacidad de separar clases en todo el rango de thresholds\n",
    "   \n",
    "3. **Confiabilidad (AUPRC)** - 25%\n",
    "   - Importante en datasets desbalanceados\n",
    "   \n",
    "4. **Accuracy General** - 20%\n",
    "   - Performance global del sistema\n",
    "\n",
    "**2.2 Ranking con Score Ponderado**\n",
    "\n",
    "| Posición | Modelo | Score Ponderado | Justificación |\n",
    "|----------|--------|-----------------|---------------|\n",
    "| **1º** | **EfficientNet-B0** | **0.8242** | Domina en todas las métricas clave excepto recall puro |\n",
    "| **2º** | **Random Forest** | **0.6848** | Mejor balance entre modelos no-CNN, buena especificidad |\n",
    "| **3º** | **Regresión Lineal** | **0.6602** | Sorpresivamente competitivo para su simplicidad |\n",
    "| 4º | SVM (HOG+PCA) | 0.6975 |  |\n",
    "| 5º | XGBoost | 0.6375 | Mejor recall pero muchos falsos positivos |\n",
    "| 6º | CatBoost | 0.6147 | Recall muy bajo lo penaliza severamente |\n",
    "\n",
    "**2.3 Justificación de la Selección**\n",
    "\n",
    "EfficientNet-B0 - Modelo Principal\n",
    "\n",
    "**Seleccionado por:**\n",
    "- **Rendimiento superior global**: Lidera en 5 de 6 métricas principales\n",
    "- **Balance óptimo**: F1=0.758 indica equilibrio precision-recall adecuado para ciencia\n",
    "- **Confiabilidad**: AUROC=0.921 y AUPRC=0.869 garantizan predicciones robustas\n",
    "- **Capacidad de generalización**: Aprende representaciones jerárquicas de morfologías galácticas\n",
    "- **Sin feature engineering**: Procesa directamente las imágenes astronómicas\n",
    "\n",
    "**Casos de uso recomendados:**\n",
    "- Análisis científico que requiere alta confiabilidad\n",
    "- Clasificación en producción con acceso a GPU\n",
    "- Estudios poblacionales de grandes surveys\n",
    "\n",
    "Random Forest - Modelo Alternativo/Complementario\n",
    "\n",
    "**Seleccionado por:**\n",
    "- **Balance sólido**: F1=0.649 mejor que otros modelos no-CNN\n",
    "- **Baja tasa de falsos positivos**: Solo 9.9% (vs 48.4% de XGBoost)\n",
    "- **AUROC competitivo**: 0.729 es razonable para método tradicional\n",
    "- **Interpretabilidad**: Análisis de importancia de features científicamente valioso\n",
    "- **Estabilidad**: Robusto ante variaciones en datos\n",
    "\n",
    "**Casos de uso recomendados:**\n",
    "- Validación cruzada de resultados de CNN\n",
    "- Análisis exploratorio de datos\n",
    "- Sistemas con recursos muy limitados\n",
    "- Escenarios donde falsos positivos son costosos\n",
    "- Baseline robusto para comparaciones futuras\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4MHA6MmI9DI"
   },
   "source": [
    "# Ajustes Aplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSPupfyXOZzF"
   },
   "source": [
    "**3.1 Ajustes para EfficientNet-B0**\n",
    "\n",
    "A) Optimización de Threshold\n",
    "```\n",
    "Threshold por defecto: 0.5\n",
    "Threshold optimizado (val): 0.256\n",
    "Mejora en F1: +3.8%\n",
    "```\n",
    "**Justificación:** El threshold óptimo se seleccionó maximizando F1-score en validación, balanceando la necesidad de detectar barras débiles sin generar excesivos falsos positivos.\n",
    "\n",
    "B) Data Augmentation Astronómico\n",
    "**Transformaciones aplicadas:**\n",
    "- Rotaciones 360° (p=1.0) - galaxias sin orientación preferente\n",
    "- Flips horizontal/vertical (p=0.5) - simetría natural\n",
    "- Random brightness/contrast (±5%, p=0.3) - variabilidad observacional\n",
    "- Gaussian noise (var=1e-5 a 5e-4, p=0.3) - ruido instrumental\n",
    "- Gaussian blur (3-5px, p=0.3) - seeing atmosférico\n",
    "\n",
    "**Impacto esperado:** +2-4% en robustez ante variaciones observacionales\n",
    "\n",
    "C) Regularización Mejorada\n",
    "```python\n",
    "- Dropout: 0.2 en capas fully-connected\n",
    "- Weight decay: 1e-4\n",
    "- Early stopping: patience=6 epochs\n",
    "- Learning rate: 2e-4 con Cosine Annealing\n",
    "```\n",
    "\n",
    "D) Arquitectura Final\n",
    "- Backbone: EfficientNet-B0 (pretrained)\n",
    "- Feature extraction: 1280 dims\n",
    "- Dual heads:\n",
    "  - Clasificación binaria: 256→1 (sigmoid)\n",
    "  - Regresión fuerza: 256→1 (sigmoid) [strength]\n",
    "\n",
    "**Mejoras totales estimadas:** 5-7% en F1-score respecto a baseline\n",
    "\n",
    "**3.2 Ajustes para Random Forest**\n",
    "\n",
    "A) Optimización de Hiperparámetros\n",
    "**Configuración final:**\n",
    "```python\n",
    "RandomForestClassifier(\n",
    "    n_estimators=800,          # Número de árboles\n",
    "    max_depth=None,            # Sin límite de profundidad\n",
    "    min_samples_split=2,       # Mínimo para split\n",
    "    min_samples_leaf=1,        # Mínimo por hoja\n",
    "    max_features='sqrt',       # Features por split\n",
    "    bootstrap=True,            # Bagging activado\n",
    "    class_weight='balanced',   # Manejo de desbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1                  # Paralelización completa\n",
    ")\n",
    "```\n",
    "\n",
    "**Proceso de optimización:**\n",
    "- Grid search sobre parámetros clave\n",
    "- Validación cruzada estratificada\n",
    "- Balance entre complejidad y generalización\n",
    "\n",
    "B) Feature Engineering (Compartido con XGBoost)\n",
    "**Features estadísticos (61 totales):**\n",
    "\n",
    "**Por banda (g, r, z) - 19 features cada una:**\n",
    "- Momentos estadísticos: mean, std, median, min, max, skewness, kurtosis\n",
    "- Percentiles robustos: q01, q05, q25, q75, q95, q99\n",
    "- Textura: varianza local (ventana 5x5)\n",
    "- Entropía: información de Shannon\n",
    "- Gradientes: magnitud (mean, std, max), dirección (mean, std)\n",
    "\n",
    "**Inter-bandas - 4 features:**\n",
    "- Correlaciones de Pearson: corr(g,r), corr(r,z), corr(g,z)\n",
    "- Ratio medio g/r (proxy de color)\n",
    "\n",
    "**Importancia de features en Random Forest (Top 10):**\n",
    "1. r_entropy (0.042) - Complejidad de estructura\n",
    "2. g_grad_max (0.038) - Gradientes fuertes (bordes)\n",
    "3. color_gmr_p50 (0.035) - Color g-r mediano\n",
    "4. z_std (0.033) - Dispersión de brillo\n",
    "5. g_texture_var (0.031) - Textura local\n",
    "6. r_p95 (0.029) - Brillo extremo\n",
    "7. corr_gr (0.028) - Correlación g-r\n",
    "8. z_skew (0.027) - Asimetría de distribución\n",
    "9. g_kurt (0.026) - Cola de distribución\n",
    "10. r_grad_std (0.025) - Variabilidad de gradientes\n",
    "\n",
    "**Diferencias con XGBoost:**\n",
    "- Random Forest valora más features de textura y gradientes\n",
    "- XGBoost prioriza momentos estadísticos (skewness, kurtosis)\n",
    "- Complementariedad en features relevantes sugiere ensemble potencial\n",
    "\n",
    "C) Manejo de Desbalance de Clases\n",
    "```python\n",
    "# Class weights automáticos\n",
    "class_weight='balanced'  \n",
    "# Equivale a: n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "# Resultante:\n",
    "# Clase 0 (Sin barra): weight = 0.798\n",
    "# Clase 1 (Con barra): weight = 1.340\n",
    "```\n",
    "\n",
    "**Impacto:** Penaliza más errores en clase minoritaria (barras)\n",
    "\n",
    "D) Threshold Optimizado\n",
    "```\n",
    "Threshold por defecto: 0.5\n",
    "Threshold optimizado (val): 0.45 (ajustado para balance)\n",
    "Mejora en F1: +2.1%\n",
    "```\n",
    "\n",
    "**Mejoras totales:** 8-10% en F1-score respecto a baseline sin tuning\n",
    "\n",
    "**3.3 Estrategia de Ensemble (Opcional)**\n",
    "\n",
    "Para combinar fortalezas de ambos modelos:\n",
    "\n",
    "```python\n",
    "# Ensemble ponderado\n",
    "prob_final = 0.7 * prob_efficientnet + 0.3 * prob_xgboost\n",
    "\n",
    "# Predicción final\n",
    "pred_final = (prob_final >= threshold_optimal)\n",
    "```\n",
    "\n",
    "**Mejora esperada:** +1-3% en F1-score, especialmente en casos límite\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkAW5VXcI-3k"
   },
   "source": [
    "# Selección de Modelo Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuBMOz2iOdfd"
   },
   "source": [
    "**4.1 Decisión: EfficientNet-B0**\n",
    "\n",
    "Después de evaluar los seis modelos, seleccionamos **EfficientNet-B0** como nuestro modelo final para la detección de barras en galaxias.\n",
    "\n",
    "**4.2 ¿Por qué EfficientNet-B0?**\n",
    "\n",
    "La decisión fue clara cuando comparamos los resultados: EfficientNet-B0 superó consistentemente a todos los demás modelos en las métricas que más importan para este problema astronómico. Con un F1-Score de 0.758 y un AUROC de 0.921, este modelo demostró ser capaz de detectar barras galácticas con alta confiabilidad, cometiendo significativamente menos errores que las alternativas.\n",
    "\n",
    "Lo que realmente distingue a EfficientNet-B0 es su capacidad para aprender automáticamente qué características definen una barra galáctica directamente de las imágenes, sin necesidad de que nosotros le digamos explícitamente qué buscar. Esto contrasta con modelos como Random Forest o XGBoost, donde tuvimos que diseñar manualmente 61 características diferentes (brillo, textura, gradientes, etc.) esperando capturar los patrones relevantes.\n",
    "\n",
    "Random Forest, nuestro segundo lugar, ofrece ventajas importantes como interpretabilidad y menor costo computacional, pero su F1-Score de 0.649 está significativamente por debajo. Más crítico aún, genera más falsos negativos (213 vs 137), lo que significa que pasa por alto más galaxias con barras, algo inaceptable cuando el objetivo científico es construir un catálogo completo y confiable.\n",
    "\n",
    "**4.3 Comparación Directa con Random Forest**\n",
    "\n",
    "| Aspecto | EfficientNet-B0 | Random Forest |\n",
    "|---------|-----------------|---------------|\n",
    "| **F1-Score** | 0.758 | 0.649 (-14.4%) |\n",
    "| **Falsos Positivos** | 96 (6.3%) | 81 (5.3%) |\n",
    "| **Falsos Negativos** | 137 (9.0%) | 213 (14.0%) |\n",
    "| **Confiabilidad científica** | Alta (AUPRC=0.869) | Buena (AUPRC=0.709) |\n",
    "| **Feature engineering** | No requerido | Manual, 61 features |\n",
    "| **Generalización** | Excelente | Buena |\n",
    "| **Interpretabilidad** | Baja | Alta |\n",
    "| **Tiempo inferencia** | 15-20 ms/imagen (GPU) | 8 ms/imagen (CPU) |\n",
    "| **Deployment** | Requiere GPU | CPU suficiente |\n",
    "\n",
    "**4.4 Limitaciones y Casos de Uso**\n",
    "\n",
    "EfficientNet-B0 requiere GPU para funcionar eficientemente y es menos interpretable que los modelos tradicionales, pero estos trade-offs valen la pena considerando el salto en rendimiento. Para aplicaciones que requieran explicabilidad inmediata o tengan recursos muy limitados, Random Forest sigue siendo una alternativa viable como modelo de respaldo.\n",
    "\n",
    "En producción, EfficientNet-B0 procesará el catálogo completo de MaNGA (~10,000 galaxias) en pocas horas, generando clasificaciones confiables que reducirán drásticamente el tiempo que los astrónomos dedican a clasificación manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmcFieOeJBkE"
   },
   "source": [
    "# Conclusiones Finales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9gHJgebQDrs"
   },
   "source": [
    "Después de evaluar seis modelos diferentes para la detección automática de barras en galaxias del survey MaNGA, seleccionamos EfficientNet-B0 como nuestro modelo final de producción. Esta decisión se fundamenta en su rendimiento claramente superior con un F1-Score de 0.758 y un AUROC de 0.921, que supera por amplio margen a las alternativas evaluadas. Aunque modelos como Random Forest ofrecen ventajas en interpretabilidad y eficiencia computacional, EfficientNet-B0 logra el balance crítico entre precisión (0.792) y recall (0.727) que necesita la astronomía moderna: detectar la mayor cantidad de barras galácticas posible sin inundar al equipo científico con falsos positivos que requieran verificación manual. Su capacidad para aprender automáticamente patrones morfológicos complejos a partir de las imágenes astronómicas, sin necesidad de diseñar features manualmente, lo convierte en la herramienta ideal para escalar este análisis a los millones de galaxias que observarán los próximos surveys. Con apenas 96 falsos positivos en 1,519 galaxias de prueba y un tiempo de inferencia de 15-20 milisegundos por imagen con GPU, estamos listos para procesar el catálogo completo de MaNGA y generar el primer catálogo homogéneo de detección de barras producido íntegramente por deep learning para la comunidad astronómica."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
